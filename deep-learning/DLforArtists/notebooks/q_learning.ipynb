{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement learning involves training agents - programs, robots, etc - such that they learn how to behave in a particular environment. Reinforcement learning agents learn by interacting with their environment and seeing what happens as a result of their actions. This is a really exciting area of artificial intelligence - here's where we get to design agents that are autonomous in both their learning and their behavior.\n",
    "\n",
    "The basic structure of a reinforcement learning problem is:\n",
    "\n",
    "- we model the world as various **states** the agent can be in.\n",
    "- the agent can take **actions** that move between these states.\n",
    "- each state has an associated **reward** (which may be negative, i.e. a \"punishment\").\n",
    "- the agent **explores** these states and learns which sequence of actions tend to lead to more rewards.\n",
    "- the agent uses what it has learned to behave in a way that maximizes reward (it **exploits** to the best of its knowledge).\n",
    "\n",
    "There are many different ways a reinforcement learning agent can be trained, but a common one is called _Q-learning_. The agent learns a function called $Q(s,a)$ which takes a state $s$ and an action $a$ and returns a value for it. This value is the _expected reward_, cumulative from this point onwards (though agents may value future rewards less than present rewards). In effect, the higher the value, the more the agent believes $a$ is a good action to take in state $s$.\n",
    "\n",
    "The way the agent actually behaves (i.e. decides what to do) is governed by what is called a _policy_. With Q-learning a common policy is a _greedy policy_ which just means take the highest valued action given by $Q(s,a)$.\n",
    "\n",
    "There are two problems in reinforcement learning that you should be familiar with:\n",
    "\n",
    "- the _credit assignment_ problem: sometimes a reward may be due to an action taken a long time ago - how do we properly assign credit to that action? Generally we can propagate rewards backwards through time, so an early action that later leads to a reward will have some of that reward assigned to it. This will be clearer when we dig into the code.\n",
    "- the _exploration vs exploitation_ problem: does the agent stick with certain rewards at the expense of possibly missing out on greater but unknown rewards (exploit)? Does the agent explore more states to find these possibly greater rewards, but at the risk of lower or negative ones (explore)? A simple approach, which we'll use here, is to set some value, called epsilon ($\\epsilon$), which can be from 0 to 1. With this $\\epsilon$ probability the agent will take a random action instead of the best one. This variation of the greed policy is called the $\\epsilon$-greedy policy.\n",
    "\n",
    "In this guide we'll put together a very simple Q-learning agent that navigates a grid world. Grid worlds consist of some hazards (which lead to negative reward) and some kind of \"treasure\" which yields positive reward. The agent then must learn how to efficiently navigate towards the treasure while avoiding the hazards.\n",
    "\n",
    "For simplicity, we are going to consider a _fully-observed_ scenario; that is, when an agent takes an action, they see all the results of it (this is contrasted to _partially-observable_ scenarios, where some results remain unknown, perhaps until later or surfacing in different ways). Our scenario will also be _deterministic_ in that an action, from a given state, always leads to the same outcome.\n",
    "\n",
    "## The environment\n",
    "\n",
    "A reinforcement learning agent needs an environment to explore and interact with, so let's create that first. While the [OpenAI Gym](https://gym.openai.com/) provides a lot of environments to get started with, I think seeing how an environment is defined makes understanding easier.\n",
    "\n",
    "For this problem we'll think in terms of _episodes_ - we place the agent somewhere in the grid, it moves around until it reaches a _terminal_ state (i.e. a position that ends the episode), which may give negative reward or a positive reward.\n",
    "\n",
    "Our grid will consist of empty spaces (valued at 0), holes (which are terminal states that give a reward of -1) and positive reward spaces (which are also terminal states). Our agent will start in a random non-terminal state and ideally learns how to find the highest value reward.\n",
    "\n",
    "We're going to tweak things a little bit and make it so that every step the agent takes also gives -1 reward. This prevents the agent from wandering around aimlessly, instead encouraging to find the shortest path to a satisfactory reward. I say \"satisfactory\" because the agent no longer is looking for the maximum reward available - if the maximum reward is 5, but it's 10 steps away, the agent will ultimately end the episode with a total reward of -5. But perhaps there's a reward tile one step away that has a reward of only 2 - the \"smart\" thing to do is to go for the closer, lesser reward, because the agent will end the episode with a total reward of +1. Our agent, if trained enough, will learn exactly these kinds of behaviors.\n",
    "\n",
    "We'll design it so that we can pass in a grid (i.e. a list of lists) that is filled with reward values. Values of `None` specify a hole in the world.\n",
    "\n",
    "For a grid world, the states are just `(x,y)` coordinate positions.\n",
    "\n",
    "The environment will also tell us what valid actions are given a state. For example, if the agent is in the upper-left corner of the map, the only valid actions are `right`, and `down`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, grid):\n",
    "        self.grid = grid\n",
    "        self.n_rows = len(grid)\n",
    "        self.n_cols = len(grid[0])\n",
    "        self.positions = self._positions()\n",
    "        self.starting_positions = [p for p in self.positions\n",
    "                                   if not self.is_terminal_state(p)]\n",
    "\n",
    "    def actions(self, pos):\n",
    "        \"\"\"possible actions for a state (position)\"\"\"\n",
    "        r, c = pos\n",
    "        actions = []\n",
    "        if r > 0:\n",
    "            actions.append('up')\n",
    "        if r < self.n_rows - 1:\n",
    "            actions.append('down')\n",
    "        if c > 0:\n",
    "            actions.append('left')\n",
    "        if c < self.n_cols - 1:\n",
    "            actions.append('right')\n",
    "        return actions\n",
    "\n",
    "    def value(self, pos):\n",
    "        \"\"\"retrieve the reward value for a position\"\"\"\n",
    "        r, c = pos\n",
    "        return self.grid[r][c]\n",
    "\n",
    "    def _positions(self):\n",
    "        \"\"\"all positions\"\"\"\n",
    "        positions = []\n",
    "        for r, row in enumerate(self.grid):\n",
    "            for c, _ in enumerate(row):\n",
    "                positions.append((r,c))\n",
    "        return positions\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"tell us if the state ends the game\"\"\"\n",
    "        val = self.value(state)\n",
    "        return val is None or val > 0\n",
    "\n",
    "    def reward(self, state):\n",
    "        \"\"\"the reward of a state:\n",
    "        -1 if it's a hole,\n",
    "        -1 if it's an empty space (to penalize each move),\n",
    "        otherwise, the value of the state\"\"\"\n",
    "        val = self.value(state)\n",
    "        if val is None or val == 0:\n",
    "            return -1\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this `Environment` class to define a grid world like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Environment([\n",
    "    [   0,  0,    0,  0,    0, None, None],\n",
    "    [   0,  0,    5,  0,    0,    0, None],\n",
    "    [   0,  0, None,  5, None,    0, None],\n",
    "    [None,  0,    5,  5, None,   10,    0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent\n",
    "\n",
    "Now we'll move on to designing the agent.\n",
    "\n",
    "A few notes here:\n",
    "\n",
    "- We have a `learning_rate` parameter here (it takes a value from 0 to 1). This is useful for stochastic (non-deterministic) environments; it allows you to control how much new information overwrites existing information about the environment. In stochastic environments, sometimes random weird things happen and you don't want it influencing the agent too much. In deterministic environment (like ours), this isn't a problem, so you want to set it to 1 so the agent learns as quickly as possible.\n",
    "- The `discount` value (from 0 to 1) specifies how much future rewards are discounted by. For instance, with `discount=0.5`, a reward at the next time step is only worth half as much as it would be now. A reward at the time step after that would be discounted twice-over, i.e. it would be worth only `0.5 * 0.5` of it's actual value.\n",
    "- Instead of implementing Q as a function, we're using a lookup table (i.e. a dictionary). This works fine for our purposes here, but just know that it can also be a learned function (this is where deep-Q learning comes in, which I'll cover in another guide). When the number of possible states gets really large (like with the game of Go) this lookup table approach becomes infeasible - there's just not enough memory to keep track of everything.\n",
    "\n",
    "The most important piece here is the `_learn` method. There you can see how these individual parts come together to update the value of an `(state, action)` pair. Note that when we propagate values from future states we are optimistic and take the maximum of those values. This is appropriate because we are using a greedy policy - we'll always choose the action that takes us to the best state, so we'll always be getting the next maximum (known) reward value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self, state, environment, rewards, discount=0.5, explore=0.5, learning_rate=1):\n",
    "        \"\"\"\n",
    "        - state: the agent's starting state\n",
    "        - rewards: a reward function, taking a state as input, or a mapping of states to a reward value\n",
    "        - discount: how much the agent values future rewards over immediate rewards\n",
    "        - explore: with what probability the agent \"explores\", i.e. chooses a random action\n",
    "        - learning_rate: how quickly the agent learns. For deterministic environments (like ours), this should be left at 1\n",
    "        \"\"\"\n",
    "        self.discount = discount\n",
    "        self.explore = explore\n",
    "        self.learning_rate = learning_rate\n",
    "        self.R = rewards.get if isinstance(rewards, dict) else rewards\n",
    "\n",
    "        # our state is just our position\n",
    "        self.state = state\n",
    "        self.reward = 0\n",
    "        self.env = environment\n",
    "\n",
    "        # initialize Q\n",
    "        self.Q = {}\n",
    "\n",
    "    def reset(self, state):\n",
    "        self.state = state\n",
    "        self.reward = 0\n",
    "\n",
    "    def actions(self, state):\n",
    "        return self.env.actions(state)\n",
    "\n",
    "    def _take_action(self, state, action):\n",
    "        r, c = state\n",
    "        if action == 'up':\n",
    "            r -= 1\n",
    "        elif action == 'down':\n",
    "            r += 1\n",
    "        elif action == 'right':\n",
    "            c += 1\n",
    "        elif action == 'left':\n",
    "            c -= 1\n",
    "\n",
    "        # return new state\n",
    "        return (r,c)\n",
    "\n",
    "    def step(self, action=None):\n",
    "        \"\"\"take an action\"\"\"\n",
    "        # check possible actions given state\n",
    "        actions = self.actions(self.state)\n",
    "\n",
    "        # if this is the first time in this state,\n",
    "        # initialize possible actions\n",
    "        if self.state not in self.Q:\n",
    "            self.Q[self.state] = {a: 0 for a in actions}\n",
    "\n",
    "        if action is None:\n",
    "            if random.random() < self.explore:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                action = self._best_action(self.state)\n",
    "        elif action not in actions:\n",
    "            raise ValueError('unrecognized action!')\n",
    "\n",
    "        # remember this state and action\n",
    "        # so we can later remember\n",
    "        # \"from this state, taking this action is this valuable\"\n",
    "        prev_state = self.state\n",
    "\n",
    "        # update state\n",
    "        self.state = self._take_action(self.state, action)\n",
    "\n",
    "        # update the previous state/action based on what we've learned\n",
    "        self._learn(prev_state, action, self.state)\n",
    "        return action\n",
    "\n",
    "    def _best_action(self, state):\n",
    "        \"\"\"choose the best action given a state\"\"\"\n",
    "        actions_rewards = list(self.Q[state].items())\n",
    "        return max(actions_rewards, key=lambda x: x[1])[0]\n",
    "\n",
    "    def _learn(self, prev_state, action, new_state):\n",
    "        \"\"\"update Q-value for the last taken action\"\"\"\n",
    "        if new_state not in self.Q:\n",
    "            self.Q[new_state] = {a: 0 for a in self.actions(new_state)}\n",
    "        reward = self.R(new_state)\n",
    "        self.reward += reward\n",
    "        self.Q[prev_state][action] = self.Q[prev_state][action] + self.learning_rate * (reward + self.discount * max(self.Q[new_state].values()) - self.Q[prev_state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the agent defined, we can try running it in our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training...\n",
      "('reward:', 5)\n",
      "('reward:', 2)\n",
      "('reward:', -1)\n",
      "('reward:', 10)\n",
      "('reward:', -2)\n",
      "('reward:', 5)\n",
      "('reward:', 5)\n",
      "('reward:', 5)\n",
      "('reward:', 9)\n",
      "('reward:', 2)\n",
      "training...\n",
      "learned Q table:\n",
      "(1, 3) -> {'down': 5.0, 'right': 4.58, 'up': 2.15, 'left': 5.0}\n",
      "(3, 0) -> {'right': 0, 'up': 0}\n",
      "(2, 1) -> {'down': 3.5, 'right': -1.0, 'up': 3.5, 'left': 0.935}\n",
      "(2, 6) -> {'down': 0, 'up': 0, 'left': 0}\n",
      "(1, 6) -> {'down': 0, 'up': 0, 'left': 0}\n",
      "(2, 5) -> {'down': 10.0, 'right': -1.0, 'up': 6.2, 'left': -1.0}\n",
      "(0, 3) -> {'down': 3.5, 'right': 3.122, 'left': 3.5}\n",
      "(1, 2) -> {'down': 0, 'right': 0, 'up': 0, 'left': 0}\n",
      "(1, 5) -> {'down': 8.0, 'right': -1.0, 'up': -1.0, 'left': 4.58}\n",
      "(3, 6) -> {'up': -1.0, 'left': 10.0}\n",
      "(2, 2) -> {'down': 0, 'right': 0, 'up': 0, 'left': 0}\n",
      "(1, 1) -> {'down': 2.15, 'right': 5.0, 'up': 2.15, 'left': 2.15}\n",
      "(3, 2) -> {'right': 0, 'up': 0, 'left': 0}\n",
      "(0, 0) -> {'down': 2.15, 'right': 2.15}\n",
      "(0, 4) -> {'down': 4.58, 'right': -1.0, 'left': 2.15}\n",
      "(1, 4) -> {'down': -1.0, 'right': 6.2, 'up': 3.122, 'left': 3.5}\n",
      "(0, 5) -> {'down': 0, 'right': 0, 'left': 0}\n",
      "(1, 0) -> {'down': 0.935, 'right': 3.5, 'up': 0.935}\n",
      "(3, 5) -> {'right': 0, 'up': 0, 'left': 0}\n",
      "(0, 1) -> {'down': 3.5, 'right': 3.5, 'left': 0.935}\n",
      "(3, 1) -> {'right': 5.0, 'up': 2.15, 'left': -1.0}\n",
      "(0, 2) -> {'down': 5.0, 'right': 2.15, 'left': 2.15}\n",
      "(2, 0) -> {'down': -1.0, 'right': 2.15, 'up': 2.15}\n",
      "(2, 3) -> {'down': 0, 'right': 0, 'up': 0, 'left': 0}\n",
      "(2, 4) -> {'down': 0, 'right': 0, 'up': 0, 'left': 0}\n",
      "after training...\n",
      "('reward:', 10)\n",
      "('reward:', 10)\n",
      "('reward:', 4)\n",
      "('reward:', 8)\n",
      "('reward:', 5)\n",
      "('reward:', 7)\n",
      "('reward:', 10)\n",
      "('reward:', 4)\n",
      "('reward:', 8)\n",
      "('reward:', 5)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# try discount=0.1 and discount=0.9\n",
    "pos = random.choice(env.starting_positions)\n",
    "agent = QLearner(pos, env, env.reward, discount=0.9, learning_rate=1)\n",
    "\n",
    "print('before training...')\n",
    "agent.explore = 0\n",
    "for i in range(10):\n",
    "    game_over = False\n",
    "    # start at a random position\n",
    "    pos = random.choice(env.starting_positions)\n",
    "    agent.reset(pos)\n",
    "    while not game_over:\n",
    "        agent.step()\n",
    "        game_over = env.is_terminal_state(agent.state)\n",
    "    print('reward:', agent.reward)\n",
    "\n",
    "print('training...')\n",
    "episodes = 500\n",
    "agent.explore = 0.5\n",
    "for i in range(episodes):\n",
    "    #print('episode:', i)\n",
    "    game_over = False\n",
    "    steps = 0\n",
    "\n",
    "    # start at a random position\n",
    "    pos = random.choice(env.starting_positions)\n",
    "    agent.reset(pos)\n",
    "    while not game_over:\n",
    "        agent.step()\n",
    "        steps += 1\n",
    "        game_over = env.is_terminal_state(agent.state)\n",
    "\n",
    "# print out the agent's Q table\n",
    "print('learned Q table:')\n",
    "for pos, vals in agent.Q.items():\n",
    "    print('{} -> {}'.format(pos, vals))\n",
    "\n",
    "# let's see how it does\n",
    "print('after training...')\n",
    "agent.explore = 0\n",
    "for i in range(10):\n",
    "    # start at a random position\n",
    "    pos = random.choice(env.starting_positions)\n",
    "    agent.reset(pos)\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        agent.step()\n",
    "        game_over = env.is_terminal_state(agent.state)\n",
    "    print('reward:', agent.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're training the agent for 500 episodes, which should be enough for it to thoroughly explore the space. If you don't train an agent enough it may fail to learn optimal behaviors - it hasn't experienced enough yet.\n",
    "\n",
    "The Q lookup table the agent has learned is a bit hard to parse. Let's visualize the policy it's learned - we'll render out the grid world and label each non-terminal tile with the agent's preferred action for that tile.\n",
    "\n",
    "(The following `Renderer` code is not important to understanding Q-learning, it's just so we can render the grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "class Renderer():\n",
    "    \"\"\"renders a grid with values (for the gridworld)\"\"\"\n",
    "    def __init__(self, grid, cell_size=60):\n",
    "        self.grid = grid\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        grid_h = len(grid)\n",
    "        grid_w = max(len(row) for row in grid)\n",
    "        self.size = (grid_w * self.cell_size, grid_h * self.cell_size)\n",
    "\n",
    "    def _draw_cell(self, x, y, fill, color, value, pos, text_padding=10):\n",
    "        self.draw.rectangle([(x, y), (x+self.cell_size, y+self.cell_size)], fill=fill)\n",
    "\n",
    "        # render text\n",
    "        y_mid = math.floor(self.cell_size/2)\n",
    "        lines = textwrap.wrap(str(value), width=15)\n",
    "        _, line_height = self.draw.textsize(lines[0], font=font)\n",
    "        height = len(lines) * line_height + (len(lines) - 1) * text_padding\n",
    "        current_height = y_mid - height/2\n",
    "\n",
    "        for line in lines:\n",
    "            w, h = self.draw.textsize(line, font=font)\n",
    "            self.draw.text((x + (self.cell_size - w)/2, y + current_height), line, font=font, fill=color)\n",
    "            current_height += h + text_padding\n",
    "\n",
    "    def render(self, pos=None):\n",
    "        \"\"\"renders the grid,\n",
    "        highlighting the specified position if there is one\"\"\"\n",
    "        self.img = Image.new('RGBA', self.size, color=(255,255,255,0))\n",
    "        self.draw = ImageDraw.Draw(self.img)\n",
    "\n",
    "        for r, row in enumerate(self.grid):\n",
    "            for c, val in enumerate(row):\n",
    "                if val is None:\n",
    "                    continue\n",
    "                fill = (220,220,220,255) if (r + c) % 2 == 0 else (225,225,225,255)\n",
    "\n",
    "                # current position\n",
    "                if pos is not None and pos == (r, c):\n",
    "                    fill = (255,255,150,255)\n",
    "                self._draw_cell(c * self.cell_size, r * self.cell_size, fill, (0,0,0,255), val, (r,c))\n",
    "\n",
    "        return self.img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned policy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAGQCAYAAABMPLOTAAAMU0lEQVR4nO3bTW7bWBpAUaZhrYMT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\n/FNMcpqduzUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as IPdisplay\n",
    "\n",
    "move_to_arrow = {\n",
    "    'right': '>',\n",
    "    'left': '<',\n",
    "    'up': '^',\n",
    "    'down': 'v'\n",
    "}\n",
    "\n",
    "def cell_label(qvals, reward, show_qvals=True):\n",
    "    # given the Q values for a state and the state's reward,\n",
    "    # output a string describing it\n",
    "    n = []\n",
    "    if not all(v == 0 for v in qvals.values()):\n",
    "        if show_qvals:\n",
    "            for k, v in qvals.items():\n",
    "                n.append('{}{:.2f}'.format(k[0].upper(), v))\n",
    "        best_move = max(qvals.keys(), key=lambda k: qvals[k])\n",
    "        n.append(move_to_arrow[best_move])\n",
    "    else:\n",
    "        n.append(str(reward) if reward is not None else 'hole')\n",
    "    return '\\n'.join(n)\n",
    "\n",
    "\n",
    "# generate the grid, with labels, to render\n",
    "grid = []\n",
    "for i, row in enumerate(env.grid):\n",
    "    grid.append([\n",
    "        cell_label(\n",
    "                agent.Q.get((i,j), {}),\n",
    "                env.value((i,j)),\n",
    "                show_qvals=False) for j, col in enumerate(row)])\n",
    "\n",
    "\n",
    "# display\n",
    "print('learned policy')\n",
    "renderer = Renderer(grid, cell_size=100)\n",
    "renderer.render().save('/tmp/gridworld.png')\n",
    "IPdisplay.Image(filename='/tmp/gridworld.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've trained your agent enough, the learned policy depicted here should look pretty reasonable. If the agent is within a few steps from the best reward, it will move towards that. If it's a little too far, it'll move towards the closer reward. And it should consistently be moving away from holes.\n",
    "\n",
    "That's the basics of reinforcement learning (Q-learning in particular) - try changing up the grid world and re-training your agent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
