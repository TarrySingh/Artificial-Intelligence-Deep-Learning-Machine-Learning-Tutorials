{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import torchvision.transforms as ttfs\n",
    "\n",
    "import torch, PIL\n",
    "import torch.nn as nn, torch.nn.functional as F, numpy as np, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from tqdm import tnrange as trange, tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil2tensor(image):\n",
    "    arr = torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n",
    "    arr = arr.view(image.size[1], image.size[0], -1)\n",
    "    arr = arr.permute(2,0,1)\n",
    "    return arr.float().div_(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset1(Dataset):\n",
    "    def __init__(self, fns, labels, classes=None):\n",
    "        if classes is None: classes = list(set(labels))\n",
    "        self.classes = classes\n",
    "        self.class2idx = {v:k for k,v in enumerate(classes)}\n",
    "        self.fns = np.array(fns)\n",
    "        self.y = [self.class2idx[o] for o in labels]\n",
    "        \n",
    "    @classmethod\n",
    "    def from_folder(cls, folder, classes=None, test_pct=0.):\n",
    "        if classes is None: classes = [cls.name for cls in find_classes(folder)]\n",
    "            \n",
    "        fns,labels = [],[]\n",
    "        for cl in classes:\n",
    "            fnames = get_image_files(folder/cl)\n",
    "            fns += fnames\n",
    "            labels += [cl] * len(fnames)\n",
    "            \n",
    "        if test_pct==0.: return cls(fns, labels, classes=classes)\n",
    "        fns,labels = np.array(fns),np.array(labels)\n",
    "        is_test = np.random.uniform(size=(len(fns),)) < test_pct\n",
    "        return (cls(fns[~is_test], labels[~is_test], classes=classes),\n",
    "            cls(fns[is_test], labels[is_test], classes=classes))\n",
    "\n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = PIL.Image.open(self.fns[i]).convert('RGB')\n",
    "        x = ttfs.Resize((sz,sz))(x)\n",
    "        x = pil2tensor(x)*2-1\n",
    "        return x,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files(c):\n",
    "    return [o for o in list(c.iterdir())\n",
    "            if not o.name.startswith('.') and not o.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 224\n",
    "DATA_PATH = Path('../data')\n",
    "PATH = DATA_PATH/'caltech101'\n",
    "\n",
    "data_mean,data_std = map(tensor, ([0.5355,0.5430,0.5280], [0.2909,0.2788,0.2979]))\n",
    "\n",
    "classes = ['airplanes','Motorbikes','Faces','watch','Leopards']\n",
    "np.random.seed(42)\n",
    "train_ds,valid_ds = ImageDataset1.from_folder(PATH, test_pct=0.2, classes=classes)\n",
    "# train_ds = ImageDataset1.from_folder(PATH, classes=classes)\n",
    "# valid_ds = ImageDataset1.from_folder(PATH, classes=classes)\n",
    "classes = train_ds.classes\n",
    "\n",
    "default_device = torch.device('cuda', 0)\n",
    "c = len(classes)\n",
    "c,len(train_ds),len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudaDataLoader():\n",
    "    def __init__(self,dl): self.dl = dl\n",
    "    def __len__(self): return len(self.dl)\n",
    "    def __iter__(self):\n",
    "        for o in self.dl: yield o[0].cuda(), o[1].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = CudaDataLoader(DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=8))\n",
    "valid_dl = CudaDataLoader(DataLoader(valid_ds, batch_size=2*64, shuffle=False, num_workers=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "\n",
    "def listify(p=None, q=None):\n",
    "    \"Makes p a list that looks like q\"\n",
    "    if p is None: p=[]\n",
    "    elif not isinstance(p, Iterable): p=[p]\n",
    "    n = q if type(q)==int else 1 if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    return p\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)\n",
    "\n",
    "def ResizeBatch(*size): return Lambda(lambda x: x.view((-1,)+size))\n",
    "def Flatten(): return Lambda(lambda x: x.view((x.size(0), -1)))\n",
    "def PoolFlatten(): return nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(n_classes, actns, kernel_szs, strides, bn=False):\n",
    "    kernel_szs = listify(kernel_szs, len(actns)-1)\n",
    "    strides    = listify(strides   , len(actns)-1)\n",
    "    layers = [conv2_relu(actns[i], actns[i+1], kernel_szs[i], stride=strides[i], bn=bn)\n",
    "        for i in range(len(strides))]\n",
    "    layers += [PoolFlatten(), nn.Linear(actns[-1], n_classes)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv2_relu(nif, nof, ks, stride, bn=False):\n",
    "    layers = [nn.Conv2d(nif, nof, ks, stride, padding=ks//2), nn.ReLU()]\n",
    "    if bn: layers.append(nn.BatchNorm2d(nof))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(): return simple_cnn(c, [3,16,16,16], 3, 2, bn=False)\n",
    "\n",
    "def loss_batch(model, xb, yb, loss_fn, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model().cuda()\n",
    "opt = optim.Adam(model.parameters(), 1e-3, betas=(0.9,0.99), weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(2, model, F.cross_entropy, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    *val_metrics,nums = zip(*[loss_batch(model, xb, yb, F.cross_entropy)\n",
    "                                for xb,yb in valid_dl])\n",
    "    val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),PATH/'model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load(PATH/'model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    *val_metrics,nums = zip(*[loss_batch(model1, xb, yb, F.cross_entropy)\n",
    "                                for xb,yb in valid_dl])\n",
    "    val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    *val_metrics,nums = zip(*[loss_batch(model1, xb, yb, F.cross_entropy)\n",
    "                                for xb,yb in valid_dl])\n",
    "    val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
