{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.models.cifar10.wideresnet import wrn_22\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PATH = Path(\"../data/cifar10/\")\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n",
    "\n",
    "bs=128\n",
    "sz=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 32, aug_tfms=[RandomCrop(32), RandomFlip()], pad=4)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard DawnBench result with one GPU: 94% accuracy in 22min47s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline + openCV (like in old fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(folder):\n",
    "    classes = [d for d in folder.iterdir()\n",
    "               if d.is_dir() and not d.name.startswith('.')]\n",
    "    classes.sort(key=lambda d: d.name)\n",
    "    return classes\n",
    "\n",
    "def get_image_files(c):\n",
    "    return [o for o in list(c.iterdir())\n",
    "            if not o.name.startswith('.') and not o.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as Dataset1\n",
    "\n",
    "class ImageDataset1(Dataset1):#Renamed to avoid conflict with fastai ImageDataset\n",
    "    def __init__(self, folder, tfms):\n",
    "        cls_dirs = find_classes(folder)\n",
    "        self.fns, self.y = [], []\n",
    "        self.classes = [cls.name for cls in cls_dirs]\n",
    "        for i, cls_dir in enumerate(cls_dirs):\n",
    "            fnames = get_image_files(cls_dir)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = open_image(self.fns[i])\n",
    "        for tfm in self.tfms: x,_ = tfm(x, None) \n",
    "        return x,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device, stats):\n",
    "        self.dl,self.device = dl,device\n",
    "        self.m, self.s = map(lambda x:torch.tensor(x, dtype=torch.float32, device=device), stats)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            x, y = b[0].to(self.device),b[1].to(self.device)\n",
    "            x = (x - self.m[None,:,None,None]) / self.s[None,:,None,None]\n",
    "            yield x,y\n",
    "    \n",
    "    def __len__(self): return (len(self.dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the DataLoader from pytorch since fastai replaced the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader as DataLoader1\n",
    "def get_dataloader(ds, bs, shuffle, device, stats):\n",
    "    return DeviceDataLoader(DataLoader1(ds, batch_size=bs, shuffle=shuffle,num_workers=8), device, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, trn_ds, val_ds, stats, bs=64, device=None):\n",
    "        self.device = default_device if device is None else device\n",
    "        if hasattr(trn_ds, 'classes'): self.classes = trn_ds.classes\n",
    "        self.trn_dl = get_dataloader(trn_ds, bs,   shuffle=True,  device=self.device, stats=stats)\n",
    "        self.val_dl = get_dataloader(val_ds, bs*2, shuffle=False, device=self.device, stats=stats)\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, Path, trn_tfms, val_tfms, stats, trn_name='train', val_name='valid', bs=64, device=None):\n",
    "        trn_ds, val_ds = ImageDataset1(Path/trn_name, trn_tfms), ImageDataset1(Path/val_name, val_tfms)\n",
    "        return cls(trn_ds, val_ds, stats, bs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 32, aug_tfms=[RandomCrop(32), RandomFlip()], pad=4)\n",
    "tfms[0].tfms.pop(-2)\n",
    "tfms[1].tfms.pop(-2)\n",
    "data = DataBunch.from_files(PATH, tfms[0].tfms, tfms[1].tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms1 = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms1, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataloader in pytorch is fast! It only takes 13min47s to reach the 94%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline + torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as Dataset1\n",
    "\n",
    "class ImageDataset1(Dataset1):#Renamed to avoid conflict with fastai ImageDataset\n",
    "    def __init__(self, folder, tfms):\n",
    "        cls_dirs = find_classes(folder)\n",
    "        self.fns, self.y = [], []\n",
    "        self.classes = [cls.name for cls in cls_dirs]\n",
    "        for i, cls_dir in enumerate(cls_dirs):\n",
    "            fnames = get_image_files(cls_dir)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        self.tfms = torchvision.transforms.Compose(tfms) if tfms != [] else None\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = Image.open(self.fns[i])\n",
    "        if self.tfms is not None: x = self.tfms(x) \n",
    "        return np.array(x, dtype=np.float32).transpose(2,0,1)/255,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = [torchvision.transforms.Pad(4, padding_mode='symmetric'),\n",
    "            torchvision.transforms.RandomCrop(32),\n",
    "            torchvision.transforms.RandomHorizontalFlip()]\n",
    "val_tfms = []\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a tiny bit slower but nothing remarkable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline + data aug on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do all the data aug on the torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as Dataset1\n",
    "\n",
    "class ImageDataset1(Dataset1):#Renamed to avoid conflict with fastai ImageDataset\n",
    "    def __init__(self, folder, tfms=None):\n",
    "        cls_dirs = find_classes(folder)\n",
    "        self.fns, self.y = [], []\n",
    "        self.classes = [cls.name for cls in cls_dirs]\n",
    "        for i, cls_dir in enumerate(cls_dirs):\n",
    "            fnames = get_image_files(cls_dir)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = Image.open(self.fns[i])\n",
    "        x = torch.tensor(np.array(x, dtype=np.float32).transpose(2,0,1)/255)\n",
    "        if self.tfms is not None: x = self.tfms(x)[0]\n",
    "        return x,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size):\n",
    "        self.p_flip,self.pad,self.size = p_flip,pad,size\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        x = x[:,:,a:a+self.size,b:b+self.size]\n",
    "        return do_random_flip(x, self.p_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_flip(x, prob):\n",
    "    if np.random.rand() < prob:\n",
    "        idx = [i for i in range(x.size(3)-1, -1, -1)]\n",
    "        idx = torch.LongTensor(idx)\n",
    "        return x.index_select(3, idx)\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit faster than opencv. And reflect padding instead of symmetric doesn't seem to hurt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same but with an interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(x, coords, padding='reflect'):\n",
    "    if padding=='reflect':#Reflect padding isn't implemented in grid_sample yet\n",
    "        coords[coords < -1] = coords[coords < -1].mul_(-1).add_(-2)\n",
    "        coords[coords > 1] = coords[coords > 1].mul_(-1).add_(2)\n",
    "        padding='zeros'\n",
    "    return F.grid_sample(x, coords, padding_mode=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size):\n",
    "        self.p_flip,self.pad,self.size = p_flip,pad,size\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        matrix = torch.eye(3)\n",
    "        matrix = matrix[:2,:]\n",
    "        img_size = torch.Size([1,3,h+2*self.pad,w+2*self.pad])\n",
    "        coords = F.affine_grid(matrix[None], img_size)\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        coords = coords[:,a:a+self.size,b:b+self.size,:]\n",
    "        return do_random_flip(interpolate(x, coords), self.p_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't lose time and it's still as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same with random flip as an affine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_transform(img, matrix, interpol=True, padding='reflect'):\n",
    "    \"\"\"\n",
    "    Applies an affine transformation to an image.\n",
    "    \n",
    "    Optional: only computes the new coordinates without doing the interpolation to create the new images.\n",
    "    Args:\n",
    "    x: a batch of images\n",
    "    matrix: a matrix of size 2 by 3 describing the transformation.\n",
    "            if the transformation is Ax + b, the matrix is (A|b)\n",
    "    interpol: if False, returns only the new coordinates\n",
    "    padding: padding to apply during the interpolation. Supports zeros, border, reflect\n",
    "    \n",
    "    \"\"\"\n",
    "    coords = F.affine_grid(matrix[None], img[None].size())\n",
    "    return interpolate(img[None],coords,padding) if interpol else coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_rot_matrix(degrees):\n",
    "    theta = random.uniform(-degrees,degrees) * math.pi / 180\n",
    "    return torch.tensor([[math.cos(theta), -math.sin(theta), 0],\n",
    "                         [math.sin(theta), math.cos(theta),  0],\n",
    "                         [0,               0,                1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_scale_matrix(zoom_range):\n",
    "    scale = random.uniform(*zoom_range)\n",
    "    return torch.tensor([[scale, 0, 0],\n",
    "                         [0, scale, 0],\n",
    "                         [0,  0,    1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_flip(prob):\n",
    "    if np.random.rand() < prob:\n",
    "        return torch.tensor([[-1, 0, 0],\n",
    "                             [0,  1, 0],\n",
    "                             [0,  0, 1]]).float()\n",
    "    else: return torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size):\n",
    "        self.p_flip,self.pad,self.size = p_flip,pad,size\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        matrix = get_random_flip(self.p_flip)\n",
    "        matrix = matrix[:2,:]\n",
    "        img_size = torch.Size([1,3,h+2*self.pad,w+2*self.pad])\n",
    "        coords = F.affine_grid(matrix[None], img_size)\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        coords = coords[:,a:a+self.size,b:b+self.size,:]\n",
    "        return interpolate(x, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still seems fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size, size_mult):\n",
    "        self.p_flip,self.pad,self.size,self.size_mult = p_flip,pad,size,size_mult\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        #Resize the image so that the lower dimension is size * size_mult\n",
    "        ratio = (self.size * self.size_mult) / min(h,w)\n",
    "        h,w = int(h * ratio), int(w*ratio)\n",
    "        #Pads\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        #Affine transforms\n",
    "        matrix = get_random_flip(self.p_flip)\n",
    "        matrix = matrix[:2,:]\n",
    "        img_size = torch.Size([1,3,h+2*self.pad,w+2*self.pad])\n",
    "        coords = F.affine_grid(matrix[None], img_size)\n",
    "        #Coords transforms then crop\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        coords = coords[:,a:a+self.size,b:b+self.size,:]\n",
    "        #Interpolation\n",
    "        return interpolate(x, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32, 1)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
