{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrysingh/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Eager Execution\n",
    "\n",
    "Eager execution is an experimental interface to TensorFlow that provides an imperative programming style (Ã  la, Torch7, NumPy). When you enable eager execution, TensorFlow operations execute immediately; you do not execute a pre-constructed graph with `Session.run()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Currently we're doing this... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[1, 1])\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(m, feed_dict={x: [[2.]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Getting eager execution (warning! -- Don't try this stuff in your production systems!!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-nightly\n",
      "  Downloading tf_nightly-1.5.0.dev20171122-cp36-cp36m-macosx_10_11_x86_64.whl (42.4MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.4MB 18kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tb-nightly<1.6.0a0,>=1.5.0a0 (from tf-nightly)\n",
      "  Downloading tb_nightly-1.5.0a20171121-py3-none-any.whl (2.9MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 260kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Collecting absl-py (from tf-nightly)\n",
      "  Downloading absl-py-0.1.5.tar.gz (78kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: enum34>=1.1.6 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Collecting backports.weakref>=1.0rc1 (from tf-nightly)\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting mock>=2.0.0 (from tf-nightly)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Collecting futures>=3.1.1 (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "  Downloading futures-3.1.1.tar.gz\n",
      "Requirement already satisfied: bleach==1.5.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: setuptools in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tf-nightly)\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tf-nightly)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 5.9MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, futures\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/tarrysingh/Library/Caches/pip/wheels/8c/5e/39/0c9325dbacd131f31c14f84d3e0a492358e3cc729d0bc6f1bb\n",
      "  Running setup.py bdist_wheel for futures ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/tarrysingh/Library/Caches/pip/wheels/ad/79/48/b32521764d59b16fd1bc0ffd5862f6d3bf770c7d73ea1fb12a\n",
      "Successfully built absl-py futures\n",
      "Installing collected packages: futures, tb-nightly, absl-py, backports.weakref, pbr, mock, tf-nightly\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/site-packages/pip/wheel.py\", line 323, in clobber\n",
      "    shutil.copyfile(srcfile, destfile)\n",
      "  File \"/Users/tarrysingh/anaconda/lib/python3.6/shutil.py\", line 121, in copyfile\n",
      "    with open(dst, 'wb') as fdst:\n",
      "PermissionError: [Errno 13] Permission denied: '/Users/tarrysingh/anaconda/lib/python3.6/site-packages/tensorboard/__init__.py'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed the above with the following (I hope!)\n",
    "\n",
    "```shell\n",
    "MacBook-Pro-2:~ tarrysingh$ sudo pip install tf-nightly\n",
    "Password:\n",
    "The directory '/Users/tarrysingh/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
    "The directory '/Users/tarrysingh/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
    "Collecting tf-nightly\n",
    "  Downloading tf_nightly-1.5.0.dev20171122-cp36-cp36m-macosx_10_11_x86_64.whl (42.4MB)\n",
    "    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.4MB 18kB/s \n",
    "Requirement already satisfied: enum34>=1.1.6 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting mock>=2.0.0 (from tf-nightly)\n",
    "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
    "    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 6.0MB/s \n",
    "Requirement already satisfied: protobuf>=3.4.0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting backports.weakref>=1.0rc1 (from tf-nightly)\n",
    "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
    "Requirement already satisfied: six>=1.10.0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: tb-nightly<1.6.0a0,>=1.5.0a0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: numpy>=1.12.1 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: wheel>=0.26 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting absl-py (from tf-nightly)\n",
    "Collecting pbr>=0.11 (from mock>=2.0.0->tf-nightly)\n",
    "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
    "    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 7.4MB/s \n",
    "Requirement already satisfied: setuptools in ./anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tf-nightly)\n",
    "Requirement already satisfied: werkzeug>=0.11.10 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: markdown>=2.6.8 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: html5lib==0.9999999 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: futures>=3.1.1 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: bleach==1.5.0 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Installing collected packages: pbr, mock, backports.weakref, absl-py, tf-nightly\n",
    "Successfully installed absl-py-0.1.5 backports.weakref-1.0.post1 mock-2.0.0 pbr-3.1.1 tf-nightly-1.5.0.dev20171122\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0-dev20171122\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are playing with fire here. Version 1.5!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple 3 X 3 matrix\n",
    "x = tf.matmul([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9]\n",
    "              ],\n",
    "              [\n",
    "               [10, 11, 12],\n",
    "               [13, 14, 15],\n",
    "               [16, 17, 18],\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add one to each element using tf.add for broadcasting\n",
    "y = tf.add(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a random random matrix of, say 6 by 4 matrix\n",
    "z = tf.random_uniform([6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 84  90  96]\n",
      " [201 216 231]\n",
      " [318 342 366]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 85  91  97]\n",
      " [202 217 232]\n",
      " [319 343 367]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.47207558  0.70240712  0.07745922  0.35342956]\n",
      " [ 0.74914169  0.53220427  0.12502646  0.70724893]\n",
      " [ 0.01018655  0.76463735  0.61849976  0.54388297]\n",
      " [ 0.24877775  0.21197641  0.87039661  0.91248119]\n",
      " [ 0.26888895  0.27225351  0.40576231  0.69011891]\n",
      " [ 0.57791257  0.25492835  0.05408299  0.32339919]], shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool huh!\n",
    "\n",
    "What you see is that with eager execution enabled, these ops consume and return multi-dimensional arrays as `Tensor` objects pretty much like what you see in `Torch` or `Numpy ndarray` or other imperative constructs.\n",
    "\n",
    "[**its experimental, so don't go about playing with it in production!**] there I said it again ðŸ˜Œ\n",
    "\n",
    "These operations can also be triggered via operator overloading of the `Tensor` object.\n",
    "\n",
    "Col thing is that you can use the `+` instead of tf.add, `-` for tf.subtract etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = (tf.ones([1], dtype=tf.float32) + 1) *2 - 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28, shape=(1,), dtype=float32, numpy=array([ 3.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # here you get more info about the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverting to and from Numpy\n",
    "\n",
    "This operations converts python objects to `Tensor` objects and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(1, 1)                             # tf.Tensor with a value of 2\n",
    "y = tf.add(np.array(1), np.array(1))         # tf.Tensor w a value of 2\n",
    "z = np.multiply(x, y)                        # numpu.int64 with a value of 4\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can explicitly convert using...\n",
    "\n",
    "`tf.constant` as you see next...\n",
    "\n",
    "You can also call `numpy()` method of a `Tensor` object to obtain its Numpy `ndarray` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "np_x = np.array(2., dtype=np.float32)\n",
    "x = tf.constant(np_x)\n",
    "py_y = 3.\n",
    "y = tf.constant(py_y)\n",
    "\n",
    "# add them up + 1\n",
    "\n",
    "z = x + y + 1\n",
    "\n",
    "print(z)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs GPU acceleration and telling Eager Exec where to offload the computation\n",
    "\n",
    "Easiest way to do this is to enclose your computation in ith a `tf.device('/gpu:0')` block. In case you have multiple GPUs, this function might come handy `tfe.num_gpus()`, that should return the number of GPUs you have on your cluster.\n",
    "\n",
    "Here we multiply 1000 x 1000 matrices on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Took 0.2117598056793213 seconds to multiply a (1000, 1000) matrix by itself 10 times\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure(x):\n",
    "  # The very first time a GPU is used by TensorFlow, it is initialized.\n",
    "  # So exclude the first run from timing.\n",
    "  tf.matmul(x, x)\n",
    "\n",
    "  start = time.time()\n",
    "  for i in range(10):\n",
    "    tf.matmul(x, x)\n",
    "  end = time.time()\n",
    "\n",
    "  return \"Took %s seconds to multiply a %s matrix by itself 10 times\" % (end - start, x.shape)\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  print(\"CPU: %s\" % measure(tf.random_normal([1000, 1000])))\n",
    "\n",
    "# If a GPU is available, run on GPU:\n",
    "if tfe.num_gpus() > 0:\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: %s\" % measure(tf.random_normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error copying tensor to device: GPU:0. GPU:0 unknown device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-101a482ca983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_gpu0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mgpu\u001b[0;34m(self, gpu_index)\u001b[0m\n\u001b[1;32m    756\u001b[0m       \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GPU:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_copy\u001b[0;34m(self, ctx, device_name)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mnew_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error copying tensor to device: GPU:0. GPU:0 unknown device."
     ]
    }
   ],
   "source": [
    "# you can copy Tensors to different devices\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "x_gpu0 = x.gpu()\n",
    "x_cpu = x.cpu()\n",
    "\n",
    "_ = tf.matmul(x_cpu, x_cpu) # will run this operation on CPU\n",
    "_ = tf.matmul(x_gpu0, x_gpu0)# will run on your first GPU device, you can chang it to gpu1, gpu2 if you have many\n",
    "\n",
    "if tfe.num_gpus() > 1:\n",
    "    x_gpu1 = c.gpu()\n",
    "    _ = tf.matmul(x_gpu1, x_gpu1)  # will run on your second GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I get an error above since I don't have a GPU on my Mac \n",
    "\n",
    "##  Automatic Differentiation with Eager Execution\n",
    "\n",
    "As per [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation):\n",
    "\n",
    "In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation,[1][2] is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "Automatic differentiation is not:\n",
    "- Symbolic differentiation, nor\n",
    "- Numerical differentiation (the method of finite differences).\n",
    "\n",
    "These classical methods run into problems: symbolic differentiation leads to inefficient code (unless carefully done) and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems, at the expense of introducing more software dependencies.\n",
    "\n",
    "<img src=\"images/AutomaticDifferentiationNutshell.png\">\n",
    "\n",
    "**Automatic Differentiation** in TensorFlow is useful when implementing several deep learning algorithms. Eager Execution provides an autograd-like API for automatic differentiation, especially for functions such as these:\n",
    "\n",
    "- `tfe.gradients_function(f)` : This returns a python function that computes the derivatives of the python function `f` w.r.t its arguments. `f` should return a scalr value. When invoeked this function returns a list of `Tensor` objects (an element for each argument of `f`)\n",
    "- `tfe.value_and_gradients_function(f)` : Just like the above function this one returns , when invoked, the value of `f` in addition to the list of derivatives of `f` w.r.t its arguments.\n",
    "\n",
    "Lets take a look how this works with an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return tf.multiply(x, x)\n",
    "assert 9 == f(3.).numpy\n",
    "\n",
    "# first order derivative\n",
    "\n",
    "df = tfe.gradients_function(f)\n",
    "assert 6 == df(3.)[0].numpy()\n",
    "\n",
    "# second order derivative\n",
    "d2f = tfe.gradients_function(lambda x:df(x)[0])\n",
    "assert 2 == d2f(3.)[0]"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
