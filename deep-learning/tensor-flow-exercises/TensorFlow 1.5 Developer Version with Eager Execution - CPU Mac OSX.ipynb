{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Eager Execution\n",
    "\n",
    "Eager execution is an experimental interface to TensorFlow that provides an imperative programming style (Ã  la, Torch7, NumPy). When you enable eager execution, TensorFlow operations execute immediately; you do not execute a pre-constructed graph with `Session.run()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Currently we're doing this... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[1, 1])\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(m, feed_dict={x: [[2.]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Getting eager execution (warning! -- Don't try this stuff in your production systems!!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-nightly in /Users/tarrysingh/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: enum34>=1.1.6 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: tb-nightly<1.6.0a0,>=1.5.0a0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: absl-py in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: mock>=2.0.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: backports.weakref>=1.0rc1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: setuptools in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tf-nightly)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: futures>=3.1.1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: bleach==1.5.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: pbr>=0.11 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from mock>=2.0.0->tf-nightly)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed the above with the following (I hope!)\n",
    "\n",
    "```shell\n",
    "MacBook-Pro-2:~ tarrysingh$ sudo pip install tf-nightly\n",
    "Password:\n",
    "The directory '/Users/tarrysingh/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
    "The directory '/Users/tarrysingh/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
    "Collecting tf-nightly\n",
    "  Downloading tf_nightly-1.5.0.dev20171122-cp36-cp36m-macosx_10_11_x86_64.whl (42.4MB)\n",
    "    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.4MB 18kB/s \n",
    "Requirement already satisfied: enum34>=1.1.6 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting mock>=2.0.0 (from tf-nightly)\n",
    "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
    "    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 6.0MB/s \n",
    "Requirement already satisfied: protobuf>=3.4.0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting backports.weakref>=1.0rc1 (from tf-nightly)\n",
    "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
    "Requirement already satisfied: six>=1.10.0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: tb-nightly<1.6.0a0,>=1.5.0a0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: numpy>=1.12.1 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: wheel>=0.26 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting absl-py (from tf-nightly)\n",
    "Collecting pbr>=0.11 (from mock>=2.0.0->tf-nightly)\n",
    "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
    "    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 7.4MB/s \n",
    "Requirement already satisfied: setuptools in ./anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tf-nightly)\n",
    "Requirement already satisfied: werkzeug>=0.11.10 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: markdown>=2.6.8 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: html5lib==0.9999999 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: futures>=3.1.1 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: bleach==1.5.0 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Installing collected packages: pbr, mock, backports.weakref, absl-py, tf-nightly\n",
    "Successfully installed absl-py-0.1.5 backports.weakref-1.0.post1 mock-2.0.0 pbr-3.1.1 tf-nightly-1.5.0.dev20171122\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0-dev20171122\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are playing with fire here. Version 1.5!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple 3 X 3 matrix\n",
    "x = tf.matmul([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9]\n",
    "              ],\n",
    "              [\n",
    "               [10, 11, 12],\n",
    "               [13, 14, 15],\n",
    "               [16, 17, 18],\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add one to each element using tf.add for broadcasting\n",
    "y = tf.add(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a random random matrix of, say 6 by 4 matrix\n",
    "z = tf.random_uniform([6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x) # without eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 84  90  96]\n",
      " [201 216 231]\n",
      " [318 342 366]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x) # with eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 85  91  97]\n",
      " [202 217 232]\n",
      " [319 343 367]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(y) # with eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.13902152  0.30357194  0.55132675  0.29358554]\n",
      " [ 0.61529016  0.69093883  0.00179088  0.44634724]\n",
      " [ 0.97679567  0.4461503   0.75236928  0.0593257 ]\n",
      " [ 0.77715838  0.91196513  0.49345231  0.98986089]\n",
      " [ 0.67009616  0.59602785  0.50252724  0.15992355]\n",
      " [ 0.0602212   0.45736706  0.70411134  0.56758451]], shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(z) #with eager exec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool huh!\n",
    "\n",
    "What you see is that with eager execution enabled, these ops consume and return multi-dimensional arrays as `Tensor` objects pretty much like what you see in `Torch` or `Numpy ndarray` or other imperative constructs.\n",
    "\n",
    "[**its experimental, so don't go about playing with it in production!**] there I said it again ðŸ˜Œ\n",
    "\n",
    "These operations can also be triggered via operator overloading of the `Tensor` object.\n",
    "\n",
    "Col thing is that you can use the `+` instead of tf.add, `-` for tf.subtract etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = (tf.ones([1], dtype=tf.float32) + 1) *2 - 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=20, shape=(1,), dtype=float32, numpy=array([ 3.], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # here you get more info about the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverting to and from Numpy\n",
    "\n",
    "This operations converts python objects to `Tensor` objects and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(1, 1)                             # tf.Tensor with a value of 2\n",
    "y = tf.add(np.array(1), np.array(1))         # tf.Tensor w a value of 2\n",
    "z = np.multiply(x, y)                        # numpu.int64 with a value of 4\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can explicitly convert using...\n",
    "\n",
    "`tf.constant` as you see next...\n",
    "\n",
    "You can also call `numpy()` method of a `Tensor` object to obtain its Numpy `ndarray` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "np_x = np.array(2., dtype=np.float32)\n",
    "x = tf.constant(np_x)\n",
    "py_y = 3.\n",
    "y = tf.constant(py_y)\n",
    "\n",
    "# add them up + 1\n",
    "\n",
    "z = x + y + 1\n",
    "\n",
    "print(z)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs GPU acceleration and telling Eager Exec where to offload the computation\n",
    "\n",
    "Easiest way to do this is to enclose your computation in ith a `tf.device('/gpu:0')` block. In case you have multiple GPUs, this function might come handy `tfe.num_gpus()`, that should return the number of GPUs you have on your cluster.\n",
    "\n",
    "Here we multiply 1000 x 1000 matrices on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Took 0.2379150390625 seconds to multiply a (1000, 1000) matrix by itself 10 times\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure(x):\n",
    "  # The very first time a GPU is used by TensorFlow, it is initialized.\n",
    "  # So exclude the first run from timing.\n",
    "  tf.matmul(x, x)\n",
    "\n",
    "  start = time.time()\n",
    "  for i in range(10):\n",
    "    tf.matmul(x, x)\n",
    "  end = time.time()\n",
    "\n",
    "  return \"Took %s seconds to multiply a %s matrix by itself 10 times\" % (end - start, x.shape)\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  print(\"CPU: %s\" % measure(tf.random_normal([1000, 1000])))\n",
    "\n",
    "# If a GPU is available, run on GPU:\n",
    "if tfe.num_gpus() > 0:\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: %s\" % measure(tf.random_normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error copying tensor to device: GPU:0. GPU:0 unknown device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-101a482ca983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_gpu0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mgpu\u001b[0;34m(self, gpu_index)\u001b[0m\n\u001b[1;32m    756\u001b[0m       \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GPU:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_copy\u001b[0;34m(self, ctx, device_name)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mnew_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error copying tensor to device: GPU:0. GPU:0 unknown device."
     ]
    }
   ],
   "source": [
    "# you can copy Tensors to different devices\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "x_gpu0 = x.gpu()\n",
    "x_cpu = x.cpu()\n",
    "\n",
    "_ = tf.matmul(x_cpu, x_cpu) # will run this operation on CPU\n",
    "_ = tf.matmul(x_gpu0, x_gpu0)# will run on your first GPU device, you can chang it to gpu1, gpu2 if you have many\n",
    "\n",
    "if tfe.num_gpus() > 1:\n",
    "    x_gpu1 = c.gpu()\n",
    "    _ = tf.matmul(x_gpu1, x_gpu1)  # will run on your second GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BTW I get an error above ðŸ‘† since I don't have a GPU on my Mac \n",
    "\n",
    "##  Automatic Differentiation with Eager Execution\n",
    "\n",
    "As per [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation):\n",
    "\n",
    "In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation,[1][2] is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "Automatic differentiation is not:\n",
    "- Symbolic differentiation, nor\n",
    "- Numerical differentiation (the method of finite differences).\n",
    "\n",
    "These classical methods run into problems: symbolic differentiation leads to inefficient code (unless carefully done) and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems, at the expense of introducing more software dependencies.\n",
    "\n",
    "<img src=\"images/AutomaticDifferentiationNutshell.png\">\n",
    "\n",
    "**Automatic Differentiation** in TensorFlow is useful when implementing several deep learning algorithms. Eager Execution provides an autograd-like API for automatic differentiation, especially for functions such as these:\n",
    "\n",
    "- `tfe.gradients_function(f)` : This returns a python function that computes the derivatives of the python function `f` w.r.t its arguments. `f` should return a scalr value. When invoeked this function returns a list of `Tensor` objects (an element for each argument of `f`)\n",
    "- `tfe.value_and_gradients_function(f)` : Just like the above function this one returns , when invoked, the value of `f` in addition to the list of derivatives of `f` w.r.t its arguments.\n",
    "\n",
    "Lets take a look how this works with an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-df6df4e4bd34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Third order derivative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0md3f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0md2f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0md3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return tf.multiply(x, x)  # Or x * x\n",
    "assert 9 == f(3.).numpy()\n",
    "\n",
    "df = tfe.gradients_function(f)\n",
    "assert 6 == df(3.)[0].numpy()\n",
    "\n",
    "# Second order deriviative.\n",
    "d2f = tfe.gradients_function(lambda x : df(x)[0])\n",
    "assert 2 == d2f(3.)[0].numpy()\n",
    "\n",
    "# Third order derivative.\n",
    "d3f = tfe.gradients_function(lambda x : d2f(x)[0])\n",
    "assert 0 == d3f(3.)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still don't know why the darn Nonetype error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 70.327377\n",
      "Loss at step 0: 67.532112\n",
      "Loss at step 20: 30.218769\n",
      "Loss at step 40: 13.837612\n",
      "Loss at step 60: 6.642962\n",
      "Loss at step 80: 3.481600\n",
      "Loss at step 100: 2.091797\n",
      "Loss at step 120: 1.480482\n",
      "Loss at step 140: 1.211435\n",
      "Loss at step 160: 1.092952\n",
      "Loss at step 180: 1.040738\n",
      "Final loss: 1.018471\n",
      "W, B = 3.042722, 2.121703\n"
     ]
    }
   ],
   "source": [
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# A loss function: Mean-squared error\n",
    "def loss(weight, bias):\n",
    "    error = prediction(training_inputs, weight, bias) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Function that returns the the derivative of loss with respect to\n",
    "# weight and bias\n",
    "grad = tfe.gradients_function(loss)\n",
    "\n",
    "# Train for 200 steps (starting from some random choice for W and B, on the same\n",
    "# batch of data).\n",
    "W = 5.\n",
    "B = 10.\n",
    "learning_rate = 0.01\n",
    "print(\"Initial loss: %f\" % loss(W, B).numpy())\n",
    "for i in range(200):\n",
    "  (dW, dB) = grad(W, B)\n",
    "  W -= dW * learning_rate\n",
    "  B -= dB * learning_rate\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step %d: %f\" % (i, loss(W, B).numpy()))\n",
    "print(\"Final loss: %f\" % loss(W, B).numpy())\n",
    "print(\"W, B = %f, %f\" % (W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing gradients\n",
    "\n",
    "One may want to define custom gradients for an operation, or for a function. This may be useful for multiple reasons, including providing a more efficient or more numerically stable gradient for a sequence of operations.\n",
    "\n",
    "For example, consider the function log(1 + e^x), which commonly occurs in the computation of cross entropy and log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log( 1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# works well at x = 0\n",
    "assert 0.5 == float(grad_log1pexp(0.)[0])\n",
    "\n",
    "# Returns a 'nan' at x = 100 due to numerical instability\n",
    "import math\n",
    "assert math.isnan(float(grad_log1pexp(100.)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With eager execution , define a custom gradient to simplify the gradient expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# when x = 0\n",
    "\n",
    "assert 0.5 == float(grad_log1pexp(0.)[0])\n",
    "\n",
    "# when x = 100, it works too\n",
    "\n",
    "assert 1.0 == float(grad_log1pexp(100.)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models : Building and Training\n",
    "\n",
    "Your computation may have several parameters to be optimized. Encapsulating them into re-usable classe and objects makes the code easier to understand and follow as compared to writing several top level functions with many arguments.\n",
    "\n",
    "Eager Execution encourages eagerly :) to use the Kersd-type \"Layer\" classes in `tf.layers` module. Furthermore, you might want to apply some more sophisticated techniques to compute parameter updates, such as those in `tf.train.Optimizer` implementations.\n",
    "\n",
    "In the next section we will walk through using `Optimizer` and `Layer` APIs to build a trainable TensorFlow graphs in an **eager execution enabled** environment.\n",
    "\n",
    "\n",
    "### Variables and Optimizers\n",
    "\n",
    "`tfe.Variable` objects store mutable `Tensor` values that can be accesed during training, making automatic differentiation easy. Particularly, parameters of a model can be encapsulated in Python classes as variable!\n",
    "\n",
    "`tfe.gradient_function(f)` introduced earlier computes the derivative of `f`, which becomes a bit clunky when `f` depends on a large number of trainable parameters.\n",
    "\n",
    "`tfe.implicit_gradients` is an alternative function with some useful properties such as:\n",
    "\n",
    "- it calculates the derivative of `f` w.r.t all the `tfe.Variable`'s used by `f`\n",
    "- when the returned function is invoked, it returns a list of (gradient value, Variable object) tuples.\n",
    "\n",
    "Representing model parameters as `Variable` objects, along the use of `tfe.implicit_gradients`, typically results in a better encapsulation. \n",
    "\n",
    "For instance, the linear regression model described above can be written in a class as following -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "        \n",
    "    # Define predict function\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.881783\n",
      "Loss at step 0: 67.122467\n",
      "Loss at step 20: 30.204607\n",
      "Loss at step 40: 13.911569\n",
      "Loss at step 60: 6.715141\n",
      "Loss at step 80: 3.534178\n",
      "Loss at step 100: 2.127149\n",
      "Loss at step 120: 1.504377\n",
      "Loss at step 140: 1.228563\n",
      "Loss at step 160: 1.106341\n",
      "Loss at step 180: 1.052153\n",
      "Loss at step 200: 1.028117\n",
      "Final loss: 1.028117\n",
      "W, B = 3.01984, 2.12771\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# A baby dataset of points around 3*x + 2\n",
    "NUM_EXAMPLES     = 1000\n",
    "training_inputs  = tf.random_normal([NUM_EXAMPLES])\n",
    "noise            = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# Define a model\n",
    "model = Model()\n",
    "# Define Derivatives\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "# Define a strategy for updating the variables based on the derivatives\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Traning the loop\n",
    "print(\"Initial loss: %f\" %\n",
    "      loss(model, training_inputs, training_outputs).numpy())\n",
    "for i in range(201):\n",
    "    optimizer.apply_gradients(grad(model, training_inputs, training_outputs))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (i, loss(model, training_inputs, training_outputs).numpy()))\n",
    "# Final output\n",
    "print(\"Final loss: %f\"% loss(model, training_inputs, training_outputs). numpy())\n",
    "print(\"W, B = %s, %s\"% (model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
