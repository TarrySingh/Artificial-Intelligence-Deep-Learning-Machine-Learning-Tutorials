{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmetic preparation\n",
    "### switch off deprecation and future warnings\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "# from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "import keras.backend as K\n",
    "from keras import initializers, layers\n",
    "import kuzushiji_mnist as kmnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Capsule Layers\n",
    "\n",
    "Definitions will emerge here for all parts of Capsule Layer\n",
    "\n",
    "- Class Length\n",
    "- Class Mask\n",
    "- Squashing Function\n",
    "- Class Capsule Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "    Output shape: [None, d2]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of vectors of capsules\n",
    "            x = inputs\n",
    "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
    "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
    "\n",
    "        # masked inputs, shape = [batch_size, dim_vector]\n",
    "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
    "        return inputs_masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][-1]])\n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\" \n",
    "        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            if i != 1:\n",
    "                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n",
    "                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        # End: routing algorithm V2, static ------------------------------------------------------------#\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Capsule Network Section\n",
    "\n",
    "More details will emerge here on definition, choices, architecture and eventual code explanations.\n",
    "\n",
    "- Function Convolutional Block\n",
    "- Two versions of CapsNet (details to come later)\n",
    "- Function Margin Loss\n",
    "- Function to Train and to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(np.prod(input_shape), activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=input_shape, name='out_recon')(x_recon)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model([x, y], [out_caps, x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs',\n",
    "                               batch_size=batch_size, histogram_freq=debug)\n",
    "    checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: lr * (0.95 ** epoch))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizers.Adam(lr=lr),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., lam_recon],\n",
    "                  metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction,\n",
    "                                           horizontal_flip=True)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, batch_size, shift_fraction),\n",
    "                        steps_per_epoch=int(y_train.shape[0] / batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    # End: Training with data augmentation -----------------------------------------------------------------------#\n",
    "\n",
    "    model.save_weights(save_dir + '/trained_model.h5')\n",
    "    print('Trained model saved to \\'%s/trained_model.h5\\'' % save_dir)\n",
    "\n",
    "    from utils import plot_log\n",
    "    plot_log(save_dir + '/log.csv', show=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from utils import combine_images\n",
    "    from PIL import Image\n",
    "\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_kmnist():\n",
    "#     # the data, shuffled and split between train and test sets\n",
    "#     (x_train, y_train), (x_test, y_test) = kmnist.load_data()\n",
    "\n",
    "#     x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "#     x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "#     y_train = to_categorical(y_train.astype('float32'))\n",
    "#     y_test = to_categorical(y_test.astype('float32'))\n",
    "#     return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def load_kmnist():\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    x_train = np.load(\"../input/kmnist-train-imgs.npz\")['arr_0']\n",
    "    x_test = np.load(\"../input/kmnist-test-imgs.npz\")['arr_0']\n",
    "    y_train = np.load(\"../input/kmnist-train-labels.npz\")['arr_0']\n",
    "    y_test = np.load(\"../input/kmnist-test-labels.npz\")['arr_0']\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-5dfada242255>:88: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1486080     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          8704        mask_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 784)          803600      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_caps (Length)               (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Reshape)             (None, 28, 28, 1)    0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "batch_size     = 100\n",
    "epochs         = 200\n",
    "lam_recon      = 0.392  # 784 * 0.0005, paper uses sum of SE, here uses MSE\n",
    "num_routing    = 1      # num_routing should > 0\n",
    "shift_fraction = 0.1\n",
    "debug          = 0      # debug>0 will save weights by TensorBoard\n",
    "save_dir       ='./result'\n",
    "is_training    = 1\n",
    "weights        = None\n",
    "lr             = 0.0001\n",
    "\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = load_kmnist()\n",
    "\n",
    "# define model\n",
    "model = CapsNet(input_shape=[28, 28, 1],\n",
    "                n_class=len(np.unique(np.argmax(y_train, 1))),\n",
    "                num_routing=num_routing)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.4169 - out_caps_loss: 0.3733 - out_recon_loss: 0.1114 - out_caps_acc: 0.5257 - val_loss: 0.3505 - val_out_caps_loss: 0.3128 - val_out_recon_loss: 0.0961 - val_out_caps_acc: 0.5927\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35052, saving model to ./result/weights-01.h5\n",
      "Epoch 2/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.2495 - out_caps_loss: 0.2172 - out_recon_loss: 0.0825 - out_caps_acc: 0.7546 - val_loss: 0.2806 - val_out_caps_loss: 0.2449 - val_out_recon_loss: 0.0910 - val_out_caps_acc: 0.6885\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35052 to 0.28057, saving model to ./result/weights-02.h5\n",
      "Epoch 3/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.2031 - out_caps_loss: 0.1724 - out_recon_loss: 0.0783 - out_caps_acc: 0.8232 - val_loss: 0.2527 - val_out_caps_loss: 0.2181 - val_out_recon_loss: 0.0882 - val_out_caps_acc: 0.7330\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28057 to 0.25269, saving model to ./result/weights-03.h5\n",
      "Epoch 4/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.1768 - out_caps_loss: 0.1472 - out_recon_loss: 0.0755 - out_caps_acc: 0.8557 - val_loss: 0.2283 - val_out_caps_loss: 0.1945 - val_out_recon_loss: 0.0862 - val_out_caps_acc: 0.7668\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25269 to 0.22831, saving model to ./result/weights-04.h5\n",
      "Epoch 5/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.1587 - out_caps_loss: 0.1300 - out_recon_loss: 0.0733 - out_caps_acc: 0.8772 - val_loss: 0.2089 - val_out_caps_loss: 0.1757 - val_out_recon_loss: 0.0847 - val_out_caps_acc: 0.7956\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.22831 to 0.20888, saving model to ./result/weights-05.h5\n",
      "Epoch 6/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.1469 - out_caps_loss: 0.1189 - out_recon_loss: 0.0715 - out_caps_acc: 0.8897 - val_loss: 0.1929 - val_out_caps_loss: 0.1601 - val_out_recon_loss: 0.0838 - val_out_caps_acc: 0.8220\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.20888 to 0.19293, saving model to ./result/weights-06.h5\n",
      "Epoch 7/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.1375 - out_caps_loss: 0.1100 - out_recon_loss: 0.0701 - out_caps_acc: 0.9010 - val_loss: 0.1845 - val_out_caps_loss: 0.1520 - val_out_recon_loss: 0.0829 - val_out_caps_acc: 0.8391\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19293 to 0.18450, saving model to ./result/weights-07.h5\n",
      "Epoch 8/200\n",
      "600/600 [==============================] - 210s 350ms/step - loss: 0.1295 - out_caps_loss: 0.1025 - out_recon_loss: 0.0689 - out_caps_acc: 0.9093 - val_loss: 0.1773 - val_out_caps_loss: 0.1452 - val_out_recon_loss: 0.0819 - val_out_caps_acc: 0.8421\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18450 to 0.17729, saving model to ./result/weights-08.h5\n",
      "Epoch 9/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.1224 - out_caps_loss: 0.0959 - out_recon_loss: 0.0676 - out_caps_acc: 0.9190 - val_loss: 0.1729 - val_out_caps_loss: 0.1410 - val_out_recon_loss: 0.0813 - val_out_caps_acc: 0.8490\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.17729 to 0.17287, saving model to ./result/weights-09.h5\n",
      "Epoch 10/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.1171 - out_caps_loss: 0.0910 - out_recon_loss: 0.0666 - out_caps_acc: 0.9238 - val_loss: 0.1665 - val_out_caps_loss: 0.1351 - val_out_recon_loss: 0.0803 - val_out_caps_acc: 0.8566\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.17287 to 0.16654, saving model to ./result/weights-10.h5\n",
      "Epoch 11/200\n",
      "600/600 [==============================] - 186s 311ms/step - loss: 0.1124 - out_caps_loss: 0.0867 - out_recon_loss: 0.0656 - out_caps_acc: 0.9288 - val_loss: 0.1582 - val_out_caps_loss: 0.1270 - val_out_recon_loss: 0.0795 - val_out_caps_acc: 0.8696\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.16654 to 0.15820, saving model to ./result/weights-11.h5\n",
      "Epoch 12/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.1085 - out_caps_loss: 0.0832 - out_recon_loss: 0.0647 - out_caps_acc: 0.9323 - val_loss: 0.1546 - val_out_caps_loss: 0.1237 - val_out_recon_loss: 0.0789 - val_out_caps_acc: 0.8785\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.15820 to 0.15462, saving model to ./result/weights-12.h5\n",
      "Epoch 13/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.1051 - out_caps_loss: 0.0800 - out_recon_loss: 0.0640 - out_caps_acc: 0.9361 - val_loss: 0.1489 - val_out_caps_loss: 0.1181 - val_out_recon_loss: 0.0783 - val_out_caps_acc: 0.8809\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15462 to 0.14885, saving model to ./result/weights-13.h5\n",
      "Epoch 14/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.1021 - out_caps_loss: 0.0773 - out_recon_loss: 0.0633 - out_caps_acc: 0.9384 - val_loss: 0.1474 - val_out_caps_loss: 0.1169 - val_out_recon_loss: 0.0778 - val_out_caps_acc: 0.8837\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14885 to 0.14740, saving model to ./result/weights-14.h5\n",
      "Epoch 15/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0997 - out_caps_loss: 0.0752 - out_recon_loss: 0.0627 - out_caps_acc: 0.9407 - val_loss: 0.1433 - val_out_caps_loss: 0.1130 - val_out_recon_loss: 0.0774 - val_out_caps_acc: 0.8907\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.14740 to 0.14328, saving model to ./result/weights-15.h5\n",
      "Epoch 16/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0969 - out_caps_loss: 0.0726 - out_recon_loss: 0.0621 - out_caps_acc: 0.9445 - val_loss: 0.1419 - val_out_caps_loss: 0.1116 - val_out_recon_loss: 0.0771 - val_out_caps_acc: 0.8919\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.14328 to 0.14185, saving model to ./result/weights-16.h5\n",
      "Epoch 17/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0950 - out_caps_loss: 0.0708 - out_recon_loss: 0.0617 - out_caps_acc: 0.9463 - val_loss: 0.1377 - val_out_caps_loss: 0.1077 - val_out_recon_loss: 0.0766 - val_out_caps_acc: 0.8966\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.14185 to 0.13774, saving model to ./result/weights-17.h5\n",
      "Epoch 18/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0935 - out_caps_loss: 0.0695 - out_recon_loss: 0.0612 - out_caps_acc: 0.9479 - val_loss: 0.1353 - val_out_caps_loss: 0.1054 - val_out_recon_loss: 0.0762 - val_out_caps_acc: 0.8989\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13774 to 0.13532, saving model to ./result/weights-18.h5\n",
      "Epoch 19/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0914 - out_caps_loss: 0.0675 - out_recon_loss: 0.0608 - out_caps_acc: 0.9493 - val_loss: 0.1358 - val_out_caps_loss: 0.1059 - val_out_recon_loss: 0.0762 - val_out_caps_acc: 0.8985\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13532\n",
      "Epoch 20/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0899 - out_caps_loss: 0.0662 - out_recon_loss: 0.0605 - out_caps_acc: 0.9516 - val_loss: 0.1334 - val_out_caps_loss: 0.1037 - val_out_recon_loss: 0.0758 - val_out_caps_acc: 0.9035\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.13532 to 0.13342, saving model to ./result/weights-20.h5\n",
      "Epoch 21/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0884 - out_caps_loss: 0.0648 - out_recon_loss: 0.0602 - out_caps_acc: 0.9526 - val_loss: 0.1317 - val_out_caps_loss: 0.1021 - val_out_recon_loss: 0.0755 - val_out_caps_acc: 0.9046\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.13342 to 0.13168, saving model to ./result/weights-21.h5\n",
      "Epoch 22/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0872 - out_caps_loss: 0.0637 - out_recon_loss: 0.0598 - out_caps_acc: 0.9533 - val_loss: 0.1298 - val_out_caps_loss: 0.1003 - val_out_recon_loss: 0.0752 - val_out_caps_acc: 0.9061\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.13168 to 0.12979, saving model to ./result/weights-22.h5\n",
      "Epoch 23/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0859 - out_caps_loss: 0.0625 - out_recon_loss: 0.0596 - out_caps_acc: 0.9540 - val_loss: 0.1263 - val_out_caps_loss: 0.0969 - val_out_recon_loss: 0.0750 - val_out_caps_acc: 0.9096\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.12979 to 0.12629, saving model to ./result/weights-23.h5\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0847 - out_caps_loss: 0.0615 - out_recon_loss: 0.0593 - out_caps_acc: 0.9553 - val_loss: 0.1262 - val_out_caps_loss: 0.0969 - val_out_recon_loss: 0.0749 - val_out_caps_acc: 0.9104\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12629 to 0.12623, saving model to ./result/weights-24.h5\n",
      "Epoch 25/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0838 - out_caps_loss: 0.0606 - out_recon_loss: 0.0591 - out_caps_acc: 0.9572 - val_loss: 0.1240 - val_out_caps_loss: 0.0947 - val_out_recon_loss: 0.0747 - val_out_caps_acc: 0.9116\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12623 to 0.12401, saving model to ./result/weights-25.h5\n",
      "Epoch 26/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0828 - out_caps_loss: 0.0597 - out_recon_loss: 0.0588 - out_caps_acc: 0.9570 - val_loss: 0.1243 - val_out_caps_loss: 0.0951 - val_out_recon_loss: 0.0745 - val_out_caps_acc: 0.9110\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12401\n",
      "Epoch 27/200\n",
      "600/600 [==============================] - 213s 356ms/step - loss: 0.0817 - out_caps_loss: 0.0587 - out_recon_loss: 0.0586 - out_caps_acc: 0.9584 - val_loss: 0.1217 - val_out_caps_loss: 0.0926 - val_out_recon_loss: 0.0742 - val_out_caps_acc: 0.9135\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12401 to 0.12170, saving model to ./result/weights-27.h5\n",
      "Epoch 28/200\n",
      "600/600 [==============================] - 228s 380ms/step - loss: 0.0809 - out_caps_loss: 0.0580 - out_recon_loss: 0.0584 - out_caps_acc: 0.9602 - val_loss: 0.1210 - val_out_caps_loss: 0.0920 - val_out_recon_loss: 0.0741 - val_out_caps_acc: 0.9142\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.12170 to 0.12098, saving model to ./result/weights-28.h5\n",
      "Epoch 29/200\n",
      "600/600 [==============================] - 224s 373ms/step - loss: 0.0803 - out_caps_loss: 0.0575 - out_recon_loss: 0.0582 - out_caps_acc: 0.9590 - val_loss: 0.1222 - val_out_caps_loss: 0.0932 - val_out_recon_loss: 0.0740 - val_out_caps_acc: 0.9137\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12098\n",
      "Epoch 30/200\n",
      "600/600 [==============================] - 228s 380ms/step - loss: 0.0796 - out_caps_loss: 0.0568 - out_recon_loss: 0.0581 - out_caps_acc: 0.9599 - val_loss: 0.1192 - val_out_caps_loss: 0.0902 - val_out_recon_loss: 0.0738 - val_out_caps_acc: 0.9153\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12098 to 0.11917, saving model to ./result/weights-30.h5\n",
      "Epoch 31/200\n",
      "600/600 [==============================] - 223s 372ms/step - loss: 0.0789 - out_caps_loss: 0.0562 - out_recon_loss: 0.0578 - out_caps_acc: 0.9604 - val_loss: 0.1198 - val_out_caps_loss: 0.0909 - val_out_recon_loss: 0.0737 - val_out_caps_acc: 0.9160\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11917\n",
      "Epoch 32/200\n",
      "600/600 [==============================] - 224s 373ms/step - loss: 0.0786 - out_caps_loss: 0.0560 - out_recon_loss: 0.0577 - out_caps_acc: 0.9600 - val_loss: 0.1184 - val_out_caps_loss: 0.0895 - val_out_recon_loss: 0.0736 - val_out_caps_acc: 0.9154\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.11917 to 0.11838, saving model to ./result/weights-32.h5\n",
      "Epoch 33/200\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.0781 - out_caps_loss: 0.0555 - out_recon_loss: 0.0575 - out_caps_acc: 0.9609 - val_loss: 0.1179 - val_out_caps_loss: 0.0891 - val_out_recon_loss: 0.0735 - val_out_caps_acc: 0.9159\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.11838 to 0.11790, saving model to ./result/weights-33.h5\n",
      "Epoch 34/200\n",
      "600/600 [==============================] - 223s 372ms/step - loss: 0.0774 - out_caps_loss: 0.0549 - out_recon_loss: 0.0574 - out_caps_acc: 0.9615 - val_loss: 0.1176 - val_out_caps_loss: 0.0888 - val_out_recon_loss: 0.0735 - val_out_caps_acc: 0.9162\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11790 to 0.11759, saving model to ./result/weights-34.h5\n",
      "Epoch 35/200\n",
      "600/600 [==============================] - 230s 383ms/step - loss: 0.0771 - out_caps_loss: 0.0546 - out_recon_loss: 0.0572 - out_caps_acc: 0.9610 - val_loss: 0.1158 - val_out_caps_loss: 0.0871 - val_out_recon_loss: 0.0732 - val_out_caps_acc: 0.9203\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11759 to 0.11578, saving model to ./result/weights-35.h5\n",
      "Epoch 36/200\n",
      "600/600 [==============================] - 224s 373ms/step - loss: 0.0761 - out_caps_loss: 0.0537 - out_recon_loss: 0.0571 - out_caps_acc: 0.9622 - val_loss: 0.1159 - val_out_caps_loss: 0.0872 - val_out_recon_loss: 0.0731 - val_out_caps_acc: 0.9188\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11578\n",
      "Epoch 37/200\n",
      "600/600 [==============================] - 227s 379ms/step - loss: 0.0759 - out_caps_loss: 0.0535 - out_recon_loss: 0.0570 - out_caps_acc: 0.9627 - val_loss: 0.1152 - val_out_caps_loss: 0.0866 - val_out_recon_loss: 0.0730 - val_out_caps_acc: 0.9195\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.11578 to 0.11523, saving model to ./result/weights-37.h5\n",
      "Epoch 38/200\n",
      "600/600 [==============================] - 222s 370ms/step - loss: 0.0759 - out_caps_loss: 0.0536 - out_recon_loss: 0.0569 - out_caps_acc: 0.9618 - val_loss: 0.1145 - val_out_caps_loss: 0.0859 - val_out_recon_loss: 0.0731 - val_out_caps_acc: 0.9227\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.11523 to 0.11453, saving model to ./result/weights-38.h5\n",
      "Epoch 39/200\n",
      "600/600 [==============================] - 231s 385ms/step - loss: 0.0753 - out_caps_loss: 0.0530 - out_recon_loss: 0.0568 - out_caps_acc: 0.9628 - val_loss: 0.1138 - val_out_caps_loss: 0.0852 - val_out_recon_loss: 0.0730 - val_out_caps_acc: 0.9205\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.11453 to 0.11384, saving model to ./result/weights-39.h5\n",
      "Epoch 40/200\n",
      "600/600 [==============================] - 191s 318ms/step - loss: 0.0749 - out_caps_loss: 0.0526 - out_recon_loss: 0.0567 - out_caps_acc: 0.9639 - val_loss: 0.1140 - val_out_caps_loss: 0.0854 - val_out_recon_loss: 0.0729 - val_out_caps_acc: 0.9208\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11384\n",
      "Epoch 41/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0744 - out_caps_loss: 0.0522 - out_recon_loss: 0.0566 - out_caps_acc: 0.9644 - val_loss: 0.1127 - val_out_caps_loss: 0.0842 - val_out_recon_loss: 0.0727 - val_out_caps_acc: 0.9228\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.11384 to 0.11268, saving model to ./result/weights-41.h5\n",
      "Epoch 42/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0738 - out_caps_loss: 0.0517 - out_recon_loss: 0.0565 - out_caps_acc: 0.9652 - val_loss: 0.1141 - val_out_caps_loss: 0.0856 - val_out_recon_loss: 0.0728 - val_out_caps_acc: 0.9219\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11268\n",
      "Epoch 43/200\n",
      "600/600 [==============================] - 210s 350ms/step - loss: 0.0739 - out_caps_loss: 0.0517 - out_recon_loss: 0.0565 - out_caps_acc: 0.9648 - val_loss: 0.1129 - val_out_caps_loss: 0.0844 - val_out_recon_loss: 0.0726 - val_out_caps_acc: 0.9230\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11268\n",
      "Epoch 44/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0735 - out_caps_loss: 0.0514 - out_recon_loss: 0.0563 - out_caps_acc: 0.9651 - val_loss: 0.1128 - val_out_caps_loss: 0.0844 - val_out_recon_loss: 0.0726 - val_out_caps_acc: 0.9220\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11268\n",
      "Epoch 45/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0732 - out_caps_loss: 0.0512 - out_recon_loss: 0.0563 - out_caps_acc: 0.9650 - val_loss: 0.1116 - val_out_caps_loss: 0.0832 - val_out_recon_loss: 0.0725 - val_out_caps_acc: 0.9233\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.11268 to 0.11159, saving model to ./result/weights-45.h5\n",
      "Epoch 46/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0731 - out_caps_loss: 0.0511 - out_recon_loss: 0.0562 - out_caps_acc: 0.9651 - val_loss: 0.1120 - val_out_caps_loss: 0.0836 - val_out_recon_loss: 0.0726 - val_out_caps_acc: 0.9229\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11159\n",
      "Epoch 47/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0728 - out_caps_loss: 0.0508 - out_recon_loss: 0.0562 - out_caps_acc: 0.9662 - val_loss: 0.1114 - val_out_caps_loss: 0.0830 - val_out_recon_loss: 0.0725 - val_out_caps_acc: 0.9224\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.11159 to 0.11142, saving model to ./result/weights-47.h5\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0725 - out_caps_loss: 0.0505 - out_recon_loss: 0.0561 - out_caps_acc: 0.9657 - val_loss: 0.1112 - val_out_caps_loss: 0.0829 - val_out_recon_loss: 0.0724 - val_out_caps_acc: 0.9238\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.11142 to 0.11125, saving model to ./result/weights-48.h5\n",
      "Epoch 49/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0724 - out_caps_loss: 0.0504 - out_recon_loss: 0.0560 - out_caps_acc: 0.9658 - val_loss: 0.1104 - val_out_caps_loss: 0.0821 - val_out_recon_loss: 0.0723 - val_out_caps_acc: 0.9246\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.11125 to 0.11044, saving model to ./result/weights-49.h5\n",
      "Epoch 50/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0724 - out_caps_loss: 0.0505 - out_recon_loss: 0.0560 - out_caps_acc: 0.9647 - val_loss: 0.1103 - val_out_caps_loss: 0.0820 - val_out_recon_loss: 0.0723 - val_out_caps_acc: 0.9246\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11044 to 0.11033, saving model to ./result/weights-50.h5\n",
      "Epoch 51/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0719 - out_caps_loss: 0.0500 - out_recon_loss: 0.0559 - out_caps_acc: 0.9659 - val_loss: 0.1097 - val_out_caps_loss: 0.0814 - val_out_recon_loss: 0.0723 - val_out_caps_acc: 0.9259\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.11033 to 0.10972, saving model to ./result/weights-51.h5\n",
      "Epoch 52/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0717 - out_caps_loss: 0.0498 - out_recon_loss: 0.0559 - out_caps_acc: 0.9660 - val_loss: 0.1098 - val_out_caps_loss: 0.0815 - val_out_recon_loss: 0.0722 - val_out_caps_acc: 0.9247\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.10972\n",
      "Epoch 53/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0713 - out_caps_loss: 0.0494 - out_recon_loss: 0.0558 - out_caps_acc: 0.9669 - val_loss: 0.1098 - val_out_caps_loss: 0.0815 - val_out_recon_loss: 0.0722 - val_out_caps_acc: 0.9262\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.10972\n",
      "Epoch 54/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0716 - out_caps_loss: 0.0497 - out_recon_loss: 0.0558 - out_caps_acc: 0.9655 - val_loss: 0.1096 - val_out_caps_loss: 0.0813 - val_out_recon_loss: 0.0721 - val_out_caps_acc: 0.9263\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.10972 to 0.10957, saving model to ./result/weights-54.h5\n",
      "Epoch 55/200\n",
      "600/600 [==============================] - 183s 306ms/step - loss: 0.0716 - out_caps_loss: 0.0498 - out_recon_loss: 0.0557 - out_caps_acc: 0.9661 - val_loss: 0.1097 - val_out_caps_loss: 0.0815 - val_out_recon_loss: 0.0721 - val_out_caps_acc: 0.9252\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.10957\n",
      "Epoch 56/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0717 - out_caps_loss: 0.0498 - out_recon_loss: 0.0557 - out_caps_acc: 0.9655 - val_loss: 0.1086 - val_out_caps_loss: 0.0803 - val_out_recon_loss: 0.0721 - val_out_caps_acc: 0.9268\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.10957 to 0.10859, saving model to ./result/weights-56.h5\n",
      "Epoch 57/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0710 - out_caps_loss: 0.0492 - out_recon_loss: 0.0557 - out_caps_acc: 0.9665 - val_loss: 0.1093 - val_out_caps_loss: 0.0811 - val_out_recon_loss: 0.0721 - val_out_caps_acc: 0.9267\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.10859\n",
      "Epoch 58/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0710 - out_caps_loss: 0.0492 - out_recon_loss: 0.0556 - out_caps_acc: 0.9663 - val_loss: 0.1091 - val_out_caps_loss: 0.0809 - val_out_recon_loss: 0.0720 - val_out_caps_acc: 0.9265\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.10859\n",
      "Epoch 59/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0712 - out_caps_loss: 0.0494 - out_recon_loss: 0.0556 - out_caps_acc: 0.9662 - val_loss: 0.1093 - val_out_caps_loss: 0.0810 - val_out_recon_loss: 0.0720 - val_out_caps_acc: 0.9271\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.10859\n",
      "Epoch 60/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0707 - out_caps_loss: 0.0489 - out_recon_loss: 0.0555 - out_caps_acc: 0.9665 - val_loss: 0.1087 - val_out_caps_loss: 0.0805 - val_out_recon_loss: 0.0719 - val_out_caps_acc: 0.9270\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.10859\n",
      "Epoch 61/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0705 - out_caps_loss: 0.0488 - out_recon_loss: 0.0555 - out_caps_acc: 0.9674 - val_loss: 0.1085 - val_out_caps_loss: 0.0803 - val_out_recon_loss: 0.0719 - val_out_caps_acc: 0.9269\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.10859 to 0.10850, saving model to ./result/weights-61.h5\n",
      "Epoch 62/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0709 - out_caps_loss: 0.0491 - out_recon_loss: 0.0555 - out_caps_acc: 0.9661 - val_loss: 0.1084 - val_out_caps_loss: 0.0802 - val_out_recon_loss: 0.0719 - val_out_caps_acc: 0.9271\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.10850 to 0.10839, saving model to ./result/weights-62.h5\n",
      "Epoch 63/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0705 - out_caps_loss: 0.0488 - out_recon_loss: 0.0555 - out_caps_acc: 0.9671 - val_loss: 0.1086 - val_out_caps_loss: 0.0804 - val_out_recon_loss: 0.0719 - val_out_caps_acc: 0.9269\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10839\n",
      "Epoch 64/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0704 - out_caps_loss: 0.0487 - out_recon_loss: 0.0555 - out_caps_acc: 0.9670 - val_loss: 0.1079 - val_out_caps_loss: 0.0797 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9275\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.10839 to 0.10787, saving model to ./result/weights-64.h5\n",
      "Epoch 65/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0701 - out_caps_loss: 0.0484 - out_recon_loss: 0.0554 - out_caps_acc: 0.9669 - val_loss: 0.1079 - val_out_caps_loss: 0.0798 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9270\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10787\n",
      "Epoch 66/200\n",
      "600/600 [==============================] - 210s 350ms/step - loss: 0.0701 - out_caps_loss: 0.0483 - out_recon_loss: 0.0554 - out_caps_acc: 0.9676 - val_loss: 0.1076 - val_out_caps_loss: 0.0794 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9271\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.10787 to 0.10758, saving model to ./result/weights-66.h5\n",
      "Epoch 67/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0695 - out_caps_loss: 0.0478 - out_recon_loss: 0.0553 - out_caps_acc: 0.9682 - val_loss: 0.1078 - val_out_caps_loss: 0.0796 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10758\n",
      "Epoch 68/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0698 - out_caps_loss: 0.0481 - out_recon_loss: 0.0553 - out_caps_acc: 0.9681 - val_loss: 0.1079 - val_out_caps_loss: 0.0798 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9277\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10758\n",
      "Epoch 69/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0701 - out_caps_loss: 0.0484 - out_recon_loss: 0.0554 - out_caps_acc: 0.9682 - val_loss: 0.1076 - val_out_caps_loss: 0.0795 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9277\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.10758\n",
      "Epoch 70/200\n",
      "600/600 [==============================] - 205s 341ms/step - loss: 0.0701 - out_caps_loss: 0.0484 - out_recon_loss: 0.0553 - out_caps_acc: 0.9673 - val_loss: 0.1078 - val_out_caps_loss: 0.0796 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9277\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.10758\n",
      "Epoch 71/200\n",
      "600/600 [==============================] - 187s 312ms/step - loss: 0.0698 - out_caps_loss: 0.0481 - out_recon_loss: 0.0553 - out_caps_acc: 0.9680 - val_loss: 0.1074 - val_out_caps_loss: 0.0793 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9277\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.10758 to 0.10740, saving model to ./result/weights-71.h5\n",
      "Epoch 72/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0696 - out_caps_loss: 0.0480 - out_recon_loss: 0.0553 - out_caps_acc: 0.9682 - val_loss: 0.1076 - val_out_caps_loss: 0.0795 - val_out_recon_loss: 0.0718 - val_out_caps_acc: 0.9284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10740\n",
      "Epoch 73/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0694 - out_caps_loss: 0.0478 - out_recon_loss: 0.0552 - out_caps_acc: 0.9679 - val_loss: 0.1073 - val_out_caps_loss: 0.0792 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.10740 to 0.10731, saving model to ./result/weights-73.h5\n",
      "Epoch 74/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0697 - out_caps_loss: 0.0480 - out_recon_loss: 0.0553 - out_caps_acc: 0.9677 - val_loss: 0.1074 - val_out_caps_loss: 0.0793 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9279\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10731\n",
      "Epoch 75/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0694 - out_caps_loss: 0.0478 - out_recon_loss: 0.0552 - out_caps_acc: 0.9681 - val_loss: 0.1069 - val_out_caps_loss: 0.0788 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.10731 to 0.10694, saving model to ./result/weights-75.h5\n",
      "Epoch 76/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0697 - out_caps_loss: 0.0480 - out_recon_loss: 0.0552 - out_caps_acc: 0.9678 - val_loss: 0.1071 - val_out_caps_loss: 0.0790 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9279\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.10694\n",
      "Epoch 77/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0693 - out_caps_loss: 0.0477 - out_recon_loss: 0.0552 - out_caps_acc: 0.9682 - val_loss: 0.1066 - val_out_caps_loss: 0.0785 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9280\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.10694 to 0.10663, saving model to ./result/weights-77.h5\n",
      "Epoch 78/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0694 - out_caps_loss: 0.0478 - out_recon_loss: 0.0552 - out_caps_acc: 0.9681 - val_loss: 0.1069 - val_out_caps_loss: 0.0788 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9278\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10663\n",
      "Epoch 79/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0692 - out_caps_loss: 0.0476 - out_recon_loss: 0.0552 - out_caps_acc: 0.9690 - val_loss: 0.1071 - val_out_caps_loss: 0.0790 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9276\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10663\n",
      "Epoch 80/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0691 - out_caps_loss: 0.0475 - out_recon_loss: 0.0551 - out_caps_acc: 0.9683 - val_loss: 0.1070 - val_out_caps_loss: 0.0789 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9283\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10663\n",
      "Epoch 81/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0694 - out_caps_loss: 0.0477 - out_recon_loss: 0.0552 - out_caps_acc: 0.9679 - val_loss: 0.1069 - val_out_caps_loss: 0.0788 - val_out_recon_loss: 0.0717 - val_out_caps_acc: 0.9286\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10663\n",
      "Epoch 82/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0695 - out_caps_loss: 0.0479 - out_recon_loss: 0.0552 - out_caps_acc: 0.9684 - val_loss: 0.1069 - val_out_caps_loss: 0.0788 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9280\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10663\n",
      "Epoch 83/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0695 - out_caps_loss: 0.0479 - out_recon_loss: 0.0552 - out_caps_acc: 0.9676 - val_loss: 0.1069 - val_out_caps_loss: 0.0788 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9281\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10663\n",
      "Epoch 84/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0689 - out_caps_loss: 0.0473 - out_recon_loss: 0.0551 - out_caps_acc: 0.9692 - val_loss: 0.1066 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9281\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10663\n",
      "Epoch 85/200\n",
      "600/600 [==============================] - 205s 341ms/step - loss: 0.0691 - out_caps_loss: 0.0475 - out_recon_loss: 0.0551 - out_caps_acc: 0.9683 - val_loss: 0.1065 - val_out_caps_loss: 0.0784 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9284\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.10663 to 0.10649, saving model to ./result/weights-85.h5\n",
      "Epoch 86/200\n",
      "600/600 [==============================] - 184s 307ms/step - loss: 0.0690 - out_caps_loss: 0.0474 - out_recon_loss: 0.0551 - out_caps_acc: 0.9684 - val_loss: 0.1065 - val_out_caps_loss: 0.0784 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9282\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10649\n",
      "Epoch 87/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0689 - out_caps_loss: 0.0474 - out_recon_loss: 0.0551 - out_caps_acc: 0.9680 - val_loss: 0.1067 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10649\n",
      "Epoch 88/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0695 - out_caps_loss: 0.0478 - out_recon_loss: 0.0551 - out_caps_acc: 0.9677 - val_loss: 0.1066 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9284\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10649\n",
      "Epoch 89/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0693 - out_caps_loss: 0.0477 - out_recon_loss: 0.0550 - out_caps_acc: 0.9674 - val_loss: 0.1066 - val_out_caps_loss: 0.0785 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10649\n",
      "Epoch 90/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0687 - out_caps_loss: 0.0471 - out_recon_loss: 0.0551 - out_caps_acc: 0.9694 - val_loss: 0.1066 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10649\n",
      "Epoch 91/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0690 - out_caps_loss: 0.0474 - out_recon_loss: 0.0551 - out_caps_acc: 0.9684 - val_loss: 0.1066 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9284\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10649\n",
      "Epoch 92/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0689 - out_caps_loss: 0.0474 - out_recon_loss: 0.0550 - out_caps_acc: 0.9684 - val_loss: 0.1066 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10649\n",
      "Epoch 93/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0690 - out_caps_loss: 0.0474 - out_recon_loss: 0.0551 - out_caps_acc: 0.9681 - val_loss: 0.1066 - val_out_caps_loss: 0.0785 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9286\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.10649\n",
      "Epoch 94/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0690 - out_caps_loss: 0.0475 - out_recon_loss: 0.0550 - out_caps_acc: 0.9684 - val_loss: 0.1065 - val_out_caps_loss: 0.0784 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.10649 to 0.10647, saving model to ./result/weights-94.h5\n",
      "Epoch 95/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0551 - out_caps_acc: 0.9686 - val_loss: 0.1066 - val_out_caps_loss: 0.0785 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10647\n",
      "Epoch 96/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9686 - val_loss: 0.1065 - val_out_caps_loss: 0.0785 - val_out_recon_loss: 0.0716 - val_out_caps_acc: 0.9286\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10647\n",
      "Epoch 97/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0687 - out_caps_loss: 0.0471 - out_recon_loss: 0.0550 - out_caps_acc: 0.9693 - val_loss: 0.1063 - val_out_caps_loss: 0.0783 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.10647 to 0.10634, saving model to ./result/weights-97.h5\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0689 - out_caps_loss: 0.0474 - out_recon_loss: 0.0550 - out_caps_acc: 0.9680 - val_loss: 0.1064 - val_out_caps_loss: 0.0783 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9283\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.10634\n",
      "Epoch 99/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0687 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9689 - val_loss: 0.1062 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9292\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.10634 to 0.10617, saving model to ./result/weights-99.h5\n",
      "Epoch 100/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9686 - val_loss: 0.1063 - val_out_caps_loss: 0.0783 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9292\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.10617\n",
      "Epoch 101/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9686 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.10617\n",
      "Epoch 102/200\n",
      "600/600 [==============================] - 184s 307ms/step - loss: 0.0690 - out_caps_loss: 0.0475 - out_recon_loss: 0.0550 - out_caps_acc: 0.9684 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.10617 to 0.10614, saving model to ./result/weights-102.h5\n",
      "Epoch 103/200\n",
      "600/600 [==============================] - 209s 349ms/step - loss: 0.0691 - out_caps_loss: 0.0476 - out_recon_loss: 0.0550 - out_caps_acc: 0.9684 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.10614\n",
      "Epoch 104/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0551 - out_caps_acc: 0.9687 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.10614\n",
      "Epoch 105/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9679 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.10614\n",
      "Epoch 106/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0689 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9685 - val_loss: 0.1062 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9285\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.10614\n",
      "Epoch 107/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0689 - out_caps_loss: 0.0474 - out_recon_loss: 0.0550 - out_caps_acc: 0.9679 - val_loss: 0.1063 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.10614\n",
      "Epoch 108/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0688 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9688 - val_loss: 0.1062 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.10614\n",
      "Epoch 109/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0688 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9689 - val_loss: 0.1063 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.10614\n",
      "Epoch 110/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0687 - out_caps_loss: 0.0471 - out_recon_loss: 0.0550 - out_caps_acc: 0.9679 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.10614 to 0.10611, saving model to ./result/weights-110.h5\n",
      "Epoch 111/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0690 - out_caps_loss: 0.0474 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9293\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.10611\n",
      "Epoch 112/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9691 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.10611\n",
      "Epoch 113/200\n",
      "600/600 [==============================] - 209s 349ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0550 - out_caps_acc: 0.9679 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9292\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.10611 to 0.10609, saving model to ./result/weights-113.h5\n",
      "Epoch 114/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9679 - val_loss: 0.1062 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.10609\n",
      "Epoch 115/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0550 - out_caps_acc: 0.9683 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.10609\n",
      "Epoch 116/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9686 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.10609 to 0.10608, saving model to ./result/weights-116.h5\n",
      "Epoch 117/200\n",
      "600/600 [==============================] - 187s 312ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9678 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.10608\n",
      "Epoch 118/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1062 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.10608\n",
      "Epoch 119/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9690 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.10608\n",
      "Epoch 120/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0689 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9683 - val_loss: 0.1061 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.10608 to 0.10607, saving model to ./result/weights-120.h5\n",
      "Epoch 121/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0689 - out_caps_loss: 0.0474 - out_recon_loss: 0.0550 - out_caps_acc: 0.9682 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.10607\n",
      "Epoch 122/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9681 - val_loss: 0.1061 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.10607 to 0.10606, saving model to ./result/weights-122.h5\n",
      "Epoch 123/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9688 - val_loss: 0.1061 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.10606\n",
      "Epoch 124/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0687 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9681 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.10606\n",
      "Epoch 125/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0685 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1061 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.10606\n",
      "Epoch 126/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0683 - out_caps_loss: 0.0468 - out_recon_loss: 0.0549 - out_caps_acc: 0.9697 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.10606\n",
      "Epoch 127/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9692 - val_loss: 0.1061 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.10606\n",
      "Epoch 128/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9698 - val_loss: 0.1062 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.10606\n",
      "Epoch 129/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9693 - val_loss: 0.1061 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.10606\n",
      "Epoch 130/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9684 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.10606 to 0.10604, saving model to ./result/weights-130.h5\n",
      "Epoch 131/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9684 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.10604 to 0.10603, saving model to ./result/weights-131.h5\n",
      "Epoch 132/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0549 - out_caps_acc: 0.9678 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.10603\n",
      "Epoch 133/200\n",
      "600/600 [==============================] - 180s 300ms/step - loss: 0.0688 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9678 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.10603 to 0.10602, saving model to ./result/weights-133.h5\n",
      "Epoch 134/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9692 - val_loss: 0.1061 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.10602\n",
      "Epoch 135/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0684 - out_caps_loss: 0.0468 - out_recon_loss: 0.0549 - out_caps_acc: 0.9689 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.10602 to 0.10600, saving model to ./result/weights-135.h5\n",
      "Epoch 136/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0549 - out_caps_acc: 0.9682 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9287\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.10600 to 0.10600, saving model to ./result/weights-136.h5\n",
      "Epoch 137/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0550 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.10600\n",
      "Epoch 138/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0550 - out_caps_acc: 0.9689 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.10600\n",
      "Epoch 139/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9684 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.10600\n",
      "Epoch 140/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0689 - out_caps_loss: 0.0473 - out_recon_loss: 0.0550 - out_caps_acc: 0.9682 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.10600 to 0.10600, saving model to ./result/weights-140.h5\n",
      "Epoch 141/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.10600\n",
      "Epoch 142/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.10600\n",
      "Epoch 143/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0550 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9288\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.10600\n",
      "Epoch 144/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.10600 to 0.10599, saving model to ./result/weights-144.h5\n",
      "Epoch 145/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0684 - out_caps_loss: 0.0468 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.10599 to 0.10598, saving model to ./result/weights-145.h5\n",
      "Epoch 146/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9688 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.10598\n",
      "Epoch 147/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0685 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9692 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.10598\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 187s 311ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9691 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.10598\n",
      "Epoch 149/200\n",
      "600/600 [==============================] - 205s 341ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.10598\n",
      "Epoch 150/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0683 - out_caps_loss: 0.0468 - out_recon_loss: 0.0549 - out_caps_acc: 0.9693 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.10598\n",
      "Epoch 151/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9691 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.10598\n",
      "Epoch 152/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9689 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.10598\n",
      "Epoch 153/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0685 - out_caps_loss: 0.0469 - out_recon_loss: 0.0550 - out_caps_acc: 0.9688 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.10598\n",
      "Epoch 154/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0550 - out_caps_acc: 0.9692 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.10598\n",
      "Epoch 155/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9691 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.10598\n",
      "Epoch 156/200\n",
      "600/600 [==============================] - 205s 342ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9682 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9292\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.10598 to 0.10598, saving model to ./result/weights-156.h5\n",
      "Epoch 157/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9684 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.10598 to 0.10597, saving model to ./result/weights-157.h5\n",
      "Epoch 158/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.10597 to 0.10596, saving model to ./result/weights-158.h5\n",
      "Epoch 159/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9693 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.10596\n",
      "Epoch 160/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0688 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.10596\n",
      "Epoch 161/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.10596 to 0.10596, saving model to ./result/weights-161.h5\n",
      "Epoch 162/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9684 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.10596 to 0.10596, saving model to ./result/weights-162.h5\n",
      "Epoch 163/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0550 - out_caps_acc: 0.9694 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9291\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.10596\n",
      "Epoch 164/200\n",
      "600/600 [==============================] - 181s 301ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.10596\n",
      "Epoch 165/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0688 - out_caps_loss: 0.0472 - out_recon_loss: 0.0550 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.10596\n",
      "Epoch 166/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9689 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.10596\n",
      "Epoch 167/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9693 - val_loss: 0.1060 - val_out_caps_loss: 0.0779 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.10596\n",
      "Epoch 168/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9692 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.10596\n",
      "Epoch 169/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0680 - out_caps_loss: 0.0465 - out_recon_loss: 0.0549 - out_caps_acc: 0.9695 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.10596\n",
      "Epoch 170/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.10596\n",
      "Epoch 171/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9683 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.10596\n",
      "Epoch 172/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0682 - out_caps_loss: 0.0467 - out_recon_loss: 0.0549 - out_caps_acc: 0.9694 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.10596\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9689 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.10596\n",
      "Epoch 174/200\n",
      "600/600 [==============================] - 209s 349ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9688 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.10596\n",
      "Epoch 175/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9692 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.10596\n",
      "Epoch 176/200\n",
      "600/600 [==============================] - 208s 346ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9682 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.10596\n",
      "Epoch 177/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9683 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.10596\n",
      "Epoch 178/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.10596\n",
      "Epoch 179/200\n",
      "600/600 [==============================] - 194s 323ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0550 - out_caps_acc: 0.9683 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.10596\n",
      "Epoch 180/200\n",
      "600/600 [==============================] - 202s 336ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.10596\n",
      "Epoch 181/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0688 - out_caps_loss: 0.0473 - out_recon_loss: 0.0549 - out_caps_acc: 0.9680 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.10596\n",
      "Epoch 182/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0684 - out_caps_loss: 0.0468 - out_recon_loss: 0.0550 - out_caps_acc: 0.9688 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.10596\n",
      "Epoch 183/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.10596\n",
      "Epoch 184/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0686 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9693 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.10596\n",
      "Epoch 185/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9683 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9290\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.10596\n",
      "Epoch 186/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.10596\n",
      "Epoch 187/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0550 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.10596\n",
      "Epoch 188/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0685 - out_caps_loss: 0.0469 - out_recon_loss: 0.0550 - out_caps_acc: 0.9692 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.10596\n",
      "Epoch 189/200\n",
      "600/600 [==============================] - 207s 345ms/step - loss: 0.0688 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9682 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.10596\n",
      "Epoch 190/200\n",
      "600/600 [==============================] - 207s 346ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9691 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.10596\n",
      "Epoch 191/200\n",
      "600/600 [==============================] - 208s 347ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.10596\n",
      "Epoch 192/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.10596\n",
      "Epoch 193/200\n",
      "600/600 [==============================] - 209s 349ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9685 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.10596\n",
      "Epoch 194/200\n",
      "600/600 [==============================] - 207s 344ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0549 - out_caps_acc: 0.9689 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.10596\n",
      "Epoch 195/200\n",
      "600/600 [==============================] - 181s 301ms/step - loss: 0.0686 - out_caps_loss: 0.0471 - out_recon_loss: 0.0550 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.10596\n",
      "Epoch 196/200\n",
      "600/600 [==============================] - 209s 348ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9684 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.10596\n",
      "Epoch 197/200\n",
      "600/600 [==============================] - 206s 343ms/step - loss: 0.0685 - out_caps_loss: 0.0470 - out_recon_loss: 0.0549 - out_caps_acc: 0.9686 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.10596\n",
      "Epoch 198/200\n",
      "600/600 [==============================] - 206s 344ms/step - loss: 0.0685 - out_caps_loss: 0.0469 - out_recon_loss: 0.0550 - out_caps_acc: 0.9680 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.10596\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 209s 349ms/step - loss: 0.0687 - out_caps_loss: 0.0472 - out_recon_loss: 0.0549 - out_caps_acc: 0.9687 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.10596\n",
      "Epoch 200/200\n",
      "600/600 [==============================] - 210s 349ms/step - loss: 0.0684 - out_caps_loss: 0.0469 - out_recon_loss: 0.0549 - out_caps_acc: 0.9691 - val_loss: 0.1060 - val_out_caps_loss: 0.0780 - val_out_recon_loss: 0.0715 - val_out_caps_acc: 0.9289\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.10596\n",
      "Trained model saved to './result/trained_model.h5'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Test acc: 0.9289\n",
      "\n",
      "Reconstructed images are saved to ./real_and_recon.png\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train or test\n",
    "if weights is not None:  # init the model weights with provided one\n",
    "        model.load_weights(weights)\n",
    "if is_training:\n",
    "        train(model=model, data=((x_train, y_train), (x_test, y_test)))\n",
    "else:  # as long as weights are given, will run testing\n",
    "    if weights is None:\n",
    "        print('No weights are provided. Will test using random initialized weights.')\n",
    "    \n",
    "test(model=model, data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
