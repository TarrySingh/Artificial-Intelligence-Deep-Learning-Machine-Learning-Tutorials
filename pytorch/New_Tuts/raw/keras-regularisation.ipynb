{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation in NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from tensorflow import keras as kr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set my plotting style\n",
    "plt.style.use(('dark_background', 'bmh'))\n",
    "plt.rc('axes', facecolor='none')\n",
    "plt.rc('figure', figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcuts\n",
    "imdb = kr.datasets.imdb\n",
    "Tokeniser = kr.preprocessing.text.Tokenizer\n",
    "models = kr.models\n",
    "layers = kr.layers\n",
    "regularisers = kr.regularizers\n",
    "constraints = kr.constraints\n",
    "EarlyStopping = kr.callbacks.EarlyStopping\n",
    "ModelCheckpoint = kr.callbacks.ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features we want\n",
    "features_nb = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=features_nb)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokeniser = Tokeniser(num_words=features_nb)\n",
    "train_features = tokeniser.sequences_to_matrix(train_data, mode='binary')\n",
    "test_features = tokeniser.sequences_to_matrix(test_data, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploring the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data set sizes\n",
    "print('train_data.shape:', train_data.shape)\n",
    "print('train_target.shape:', train_target.shape)\n",
    "print('test_data.shape:', test_data.shape)\n",
    "print('test_target.shape:', test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check format of first training sample\n",
    "print('type(train_data[0]):', type(train_data[0]))\n",
    "print('type(train_target[0]):', type(train_target[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of first 10 training samples and corresponding target\n",
    "print('Reviews length:', [len(sample) for sample in train_data[:10]])\n",
    "print('Review sentiment (bad/good):', train_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first review - machine format\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set text visualisation helper function\n",
    "def show_text(sample):\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "    print(' '.join(id_to_word[id_] for id_ in sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first review - human format\n",
    "show_text(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first review - neural net format\n",
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first review - neural net format - explanation\n",
    "print(train_features[0] * np.arange(len(train_features[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring regularisation of NN\n",
    "\n",
    "Play with the code, especially the one marked `# toggle`.  \n",
    "Start from `# toggle 0`, and then, one at the time, `# toggle 1` to `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add a Dropout layer\n",
    "# network.add(layers.Dropout(0.2))  # toggle 4\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "network.add(layers.Dense(\n",
    "    units=16, \n",
    "    activation='relu', \n",
    "#     kernel_regularizer=regularisers.l2(0.005),  # toggle 1\n",
    "#     kernel_regularizer=regularisers.l1(0.001),  # toggle 2\n",
    "#     kernel_constraint=constraints.max_norm(1),  # toggle 3\n",
    "    input_shape=(features_nb,)\n",
    "))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "network.add(layers.Dense(\n",
    "    units=16, \n",
    "#     kernel_regularizer=regularisers.l2(0.005),  # toggle 1\n",
    "#     kernel_constraint=constraints.max_norm(1),  # toggle 3\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "# Add a Dropout layer\n",
    "# network.add(layers.Dropout(0.5))  # toggle 4\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))  # Compile neural network\n",
    "\n",
    "# Compile network\n",
    "network.compile(\n",
    "    loss='binary_crossentropy',  # Cross-entropy\n",
    "    optimizer='rmsprop',  # Root Mean Square Propagation\n",
    "    metrics=['accuracy']   # Accuracy performance metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network\n",
    "history = network.fit(\n",
    "    train_features,  # Features\n",
    "    train_target,  # Target vector\n",
    "    epochs=25,  # Number of epochs\n",
    "    verbose=0,  # No output\n",
    "    batch_size=100,  # Number of observations per batch\n",
    "    validation_data=(test_features, test_target),  # Data for evaluation\n",
    "#     callbacks=[                                                                             # toggle 5\n",
    "#         EarlyStopping(monitor='val_loss', patience=2),                                      # toggle 5\n",
    "#         ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)  # toggle 5\n",
    "#     ],                                                                                      # toggle 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls  # toggle 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test accuracy histories\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epoch, train_loss)\n",
    "plt.plot(epoch, test_loss)\n",
    "# plt.plot(no_reg['epoch'], no_reg['train_loss'])  # toggle 0\n",
    "# plt.plot(no_reg['epoch'], no_reg['test_loss'])  # toggle 0\n",
    "\n",
    "plt.legend(['Train loss', 'Test loss', 'Train no-reg', 'Test no-reg'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss score')\n",
    "\n",
    "# Get training and test accuracy histories\n",
    "train_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epoch, train_accuracy)\n",
    "plt.plot(epoch, test_accuracy)\n",
    "# plt.plot(no_reg['epoch'], no_reg['train_accuracy'])  # toggle 0\n",
    "# plt.plot(no_reg['epoch'], no_reg['test_accuracy'])  # toggle 0\n",
    "\n",
    "plt.legend(['Train accuracy', 'Test accuracy', 'Train no-reg', 'Test no-reg'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "\n",
    "no_reg = {                             # toggle 0\n",
    "    'epoch': epoch,                    # toggle 0\n",
    "    'train_loss': train_loss,          # toggle 0\n",
    "    'test_loss': test_loss,            # toggle 0\n",
    "    'train_accuracy': train_accuracy,  # toggle 0\n",
    "    'test_accuracy': test_accuracy,    # toggle 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup weights\n",
    "weights = network.layers[0].get_weights()[0]  # toggle 0\n",
    "# weights_L1 = network.layers[0].get_weights()[0]  # toggle 1\n",
    "# weights_L2 = network.layers[0].get_weights()[0]  # toggle 2\n",
    "# weights_max = network.layers[0].get_weights()[0]  # toggle 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you got to toggle `# toggle 3`, execute the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show weight distribution\n",
    "plt.hist((\n",
    "    weights.reshape(-1),\n",
    "    weights_L1.reshape(-1),\n",
    "    weights_L2.reshape(-1),\n",
    "    weights_max.reshape(-1),\n",
    "), 49, range=(-.5, .5), label=(\n",
    "    'No-reg',\n",
    "    'L1',\n",
    "    'L2',\n",
    "    'Max',\n",
    "))\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
