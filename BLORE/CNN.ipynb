{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Importing TF, Datasets\n",
    "\n",
    "Getting the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2 ::   Create a bunch of helper functions for Conv2D\n",
    "\n",
    "1. Conv2D\n",
    "2. Regular functions that come along with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(X,W):\n",
    "    return tf.nn.conv2d(X, W, strides=[1,1,1,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(X):\n",
    "    return tf.nn.max_pool(X, ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1],\n",
    "                         padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation for why 1x2x2x1 `[batch, height, width, channel or depth]`\n",
    "\n",
    "Here we just want to do a 2X2 grid max pooling for height and width of the image.\n",
    "\n",
    "**Remember**: The first 1 is the batch: You don't usually want to skip over examples in your batch, or you shouldn't have included them in the first place. :)\n",
    "\n",
    "The last 1 is the depth of the convolution: You don't usually want to skip inputs, for the same reason.\n",
    "\n",
    "The conv2d operator is more general, so you could create convolutions that slide the window along other dimensions (this is what I was trying to explain), but that's not a typical use in convnets. The typical use is to use them spatially.\n",
    "\n",
    "**Why reshape to -1?** -1 is a placeholder that says \"adjust as necessary to match the size needed for the full tensor.\" It's a way of making the code be independent of the input batch size, so that you can change your pipeline and not have to adjust the batch size everywhere in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build out our CNN \n",
    "- placeholders\n",
    "- layers\n",
    "- loss fxn\n",
    "- optimizers\n",
    "- init, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add hidden layers\n",
    "x_image = tf.reshape(X, [-1, 28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 1\n",
    "# conv_1 = convolutional_layer(x_image, shape=[6, 6,1, 32])\n",
    "# conv_1_pooling = max_pool_2x2(conv_1)\n",
    "conv_1 = convolutional_layer(x_image,shape=[5,5,1,32])\n",
    "conv_1_pooling = max_pool_2x2(conv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_2 = convolutional_layer(conv_1_pooling,shape=[5, 5, 32, 64])\n",
    "conv_2_pooling = max_pool_2x2(conv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_2_flat = tf.reshape(conv_2_pooling, [-1, 7*7*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(conv_2_flat, 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation to why `7x7` and why `[-1, 7x7x64]`\n",
    "\n",
    "The 2x2 filter reduces height and width by 50% each. first as pool layer 1 to Our output tensor produced by `max_pooling2d() (pool1)` has a shape of `[batch_size, 14, 14, 32]`: the 2x2 filter reduces height and width by 50% each. and then again in pooling layer 2 to `[batch_size, 7,7,64]`, thus 28/2/2 = 7 for h and 28/2/2 = 7 for w.\n",
    "\n",
    "**Next** the flattening out thing...\n",
    "\n",
    "In the `reshape()` operation above, the -1 signifies that the batch_size dimension will be dynamically calculated based on the number of examples in our input data. Each example has `7 (pool2 height) * 7 (pool2 width) * 64 (pool2 channels) features, so we want the features dimension to have a value of 7 * 7 * 64 (3136 in total)`. The output tensor, pool2_flat, has shape `[batch_size, 3136]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, optimizer, init Var etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                               (labels=y_true,logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step0\n",
      "Accuracy i :\n",
      "0.0879\n",
      "Currently on step50\n",
      "Accuracy i :\n",
      "0.6972\n",
      "Currently on step100\n",
      "Accuracy i :\n",
      "0.8193\n",
      "Currently on step150\n",
      "Accuracy i :\n",
      "0.8786\n",
      "Currently on step200\n",
      "Accuracy i :\n",
      "0.8973\n",
      "Currently on step250\n",
      "Accuracy i :\n",
      "0.9086\n",
      "Currently on step300\n",
      "Accuracy i :\n",
      "0.9145\n",
      "Currently on step350\n",
      "Accuracy i :\n",
      "0.9299\n",
      "Currently on step400\n",
      "Accuracy i :\n",
      "0.9294\n",
      "Currently on step450\n",
      "Accuracy i :\n",
      "0.9336\n",
      "Currently on step500\n",
      "Accuracy i :\n",
      "0.9393\n",
      "Currently on step550\n",
      "Accuracy i :\n",
      "0.9428\n",
      "Currently on step600\n",
      "Accuracy i :\n",
      "0.9431\n",
      "Currently on step650\n",
      "Accuracy i :\n",
      "0.9484\n",
      "Currently on step700\n",
      "Accuracy i :\n",
      "0.9473\n",
      "Currently on step750\n",
      "Accuracy i :\n",
      "0.9491\n",
      "Currently on step800\n",
      "Accuracy i :\n",
      "0.9523\n",
      "Currently on step850\n",
      "Accuracy i :\n",
      "0.9538\n",
      "Currently on step900\n",
      "Accuracy i :\n",
      "0.9524\n",
      "Currently on step950\n",
      "Accuracy i :\n",
      "0.954\n",
      "Currently on step1000\n",
      "Accuracy i :\n",
      "0.9579\n",
      "Currently on step1050\n",
      "Accuracy i :\n",
      "0.9601\n",
      "Currently on step1100\n",
      "Accuracy i :\n",
      "0.9605\n",
      "Currently on step1150\n",
      "Accuracy i :\n",
      "0.9617\n",
      "Currently on step1200\n",
      "Accuracy i :\n",
      "0.9608\n",
      "Currently on step1250\n",
      "Accuracy i :\n",
      "0.9653\n",
      "Currently on step1300\n",
      "Accuracy i :\n",
      "0.9629\n",
      "Currently on step1350\n",
      "Accuracy i :\n",
      "0.9642\n",
      "Currently on step1400\n",
      "Accuracy i :\n",
      "0.9648\n",
      "Currently on step1450\n",
      "Accuracy i :\n",
      "0.965\n",
      "Currently on step1500\n",
      "Accuracy i :\n",
      "0.9668\n",
      "Currently on step1550\n",
      "Accuracy i :\n",
      "0.9661\n",
      "Currently on step1600\n",
      "Accuracy i :\n",
      "0.9673\n",
      "Currently on step1650\n",
      "Accuracy i :\n",
      "0.9699\n",
      "Currently on step1700\n",
      "Accuracy i :\n",
      "0.9689\n",
      "Currently on step1750\n",
      "Accuracy i :\n",
      "0.9698\n",
      "Currently on step1800\n",
      "Accuracy i :\n",
      "0.9698\n",
      "Currently on step1850\n",
      "Accuracy i :\n",
      "0.97\n",
      "Currently on step1900\n",
      "Accuracy i :\n",
      "0.9695\n",
      "Currently on step1950\n",
      "Accuracy i :\n",
      "0.9714\n",
      "Currently on step2000\n",
      "Accuracy i :\n",
      "0.9715\n",
      "Currently on step2050\n",
      "Accuracy i :\n",
      "0.967\n",
      "Currently on step2100\n",
      "Accuracy i :\n",
      "0.9721\n",
      "Currently on step2150\n",
      "Accuracy i :\n",
      "0.9723\n",
      "Currently on step2200\n",
      "Accuracy i :\n",
      "0.9733\n",
      "Currently on step2250\n",
      "Accuracy i :\n",
      "0.9731\n",
      "Currently on step2300\n",
      "Accuracy i :\n",
      "0.9746\n",
      "Currently on step2350\n",
      "Accuracy i :\n",
      "0.9753\n",
      "Currently on step2400\n",
      "Accuracy i :\n",
      "0.9746\n",
      "Currently on step2450\n",
      "Accuracy i :\n",
      "0.9769\n",
      "Currently on step2500\n",
      "Accuracy i :\n",
      "0.9742\n",
      "Currently on step2550\n",
      "Accuracy i :\n",
      "0.9762\n",
      "Currently on step2600\n",
      "Accuracy i :\n",
      "0.9751\n",
      "Currently on step2650\n",
      "Accuracy i :\n",
      "0.9751\n",
      "Currently on step2700\n",
      "Accuracy i :\n",
      "0.9749\n",
      "Currently on step2750\n",
      "Accuracy i :\n",
      "0.9749\n",
      "Currently on step2800\n",
      "Accuracy i :\n",
      "0.9759\n",
      "Currently on step2850\n",
      "Accuracy i :\n",
      "0.9781\n",
      "Currently on step2900\n",
      "Accuracy i :\n",
      "0.9775\n",
      "Currently on step2950\n",
      "Accuracy i :\n",
      "0.9783\n",
      "Currently on step3000\n",
      "Accuracy i :\n",
      "0.9773\n",
      "Currently on step3050\n",
      "Accuracy i :\n",
      "0.9768\n",
      "Currently on step3100\n",
      "Accuracy i :\n",
      "0.9794\n",
      "Currently on step3150\n",
      "Accuracy i :\n",
      "0.9776\n",
      "Currently on step3200\n",
      "Accuracy i :\n",
      "0.977\n",
      "Currently on step3250\n",
      "Accuracy i :\n",
      "0.9781\n",
      "Currently on step3300\n",
      "Accuracy i :\n",
      "0.9803\n",
      "Currently on step3350\n",
      "Accuracy i :\n",
      "0.98\n",
      "Currently on step3400\n",
      "Accuracy i :\n",
      "0.98\n",
      "Currently on step3450\n",
      "Accuracy i :\n",
      "0.9804\n",
      "Currently on step3500\n",
      "Accuracy i :\n",
      "0.9799\n",
      "Currently on step3550\n",
      "Accuracy i :\n",
      "0.979\n",
      "Currently on step3600\n",
      "Accuracy i :\n",
      "0.9802\n",
      "Currently on step3650\n",
      "Accuracy i :\n",
      "0.9805\n",
      "Currently on step3700\n",
      "Accuracy i :\n",
      "0.9773\n",
      "Currently on step3750\n",
      "Accuracy i :\n",
      "0.9798\n",
      "Currently on step3800\n",
      "Accuracy i :\n",
      "0.9797\n",
      "Currently on step3850\n",
      "Accuracy i :\n",
      "0.9815\n",
      "Currently on step3900\n",
      "Accuracy i :\n",
      "0.9811\n",
      "Currently on step3950\n",
      "Accuracy i :\n",
      "0.9814\n",
      "Currently on step4000\n",
      "Accuracy i :\n",
      "0.9815\n",
      "Currently on step4050\n",
      "Accuracy i :\n",
      "0.9826\n",
      "Currently on step4100\n",
      "Accuracy i :\n",
      "0.9812\n",
      "Currently on step4150\n",
      "Accuracy i :\n",
      "0.9825\n",
      "Currently on step4200\n",
      "Accuracy i :\n",
      "0.983\n",
      "Currently on step4250\n",
      "Accuracy i :\n",
      "0.9838\n",
      "Currently on step4300\n",
      "Accuracy i :\n",
      "0.9825\n",
      "Currently on step4350\n",
      "Accuracy i :\n",
      "0.9825\n",
      "Currently on step4400\n",
      "Accuracy i :\n",
      "0.9837\n",
      "Currently on step4450\n",
      "Accuracy i :\n",
      "0.9838\n",
      "Currently on step4500\n",
      "Accuracy i :\n",
      "0.9832\n",
      "Currently on step4550\n",
      "Accuracy i :\n",
      "0.9818\n",
      "Currently on step4600\n",
      "Accuracy i :\n",
      "0.9822\n",
      "Currently on step4650\n",
      "Accuracy i :\n",
      "0.9826\n",
      "Currently on step4700\n",
      "Accuracy i :\n",
      "0.9834\n",
      "Currently on step4750\n",
      "Accuracy i :\n",
      "0.9846\n",
      "Currently on step4800\n",
      "Accuracy i :\n",
      "0.9839\n",
      "Currently on step4850\n",
      "Accuracy i :\n",
      "0.9842\n",
      "Currently on step4900\n",
      "Accuracy i :\n",
      "0.9833\n",
      "Currently on step4950\n",
      "Accuracy i :\n",
      "0.984\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        batch_x, batch_y = mnist.train.next_batch(50)\n",
    "        \n",
    "        sess.run(train, feed_dict={X:batch_x, y_true:batch_y, hold_prob:0.3})\n",
    "        \n",
    "        # print message every 50 steps\n",
    "        if i%50 == 0:\n",
    "            print(\"Currently on step{}\".format(i))\n",
    "            print(\"Accuracy i :\")\n",
    "            # Test the train model\n",
    "            prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "            \n",
    "            print(sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                               y_true:mnist.test.labels,\n",
    "                                               hold_prob:1.0}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop, 6x6 filter == 97.75\n",
    "Adamoptimizer = 5x5 filter == 99.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
